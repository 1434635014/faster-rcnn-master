{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.layers as KL\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import importlib\n",
    "import config_old\n",
    "import math\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import utils\n",
    "\n",
    "importlib.reload(config_old)\n",
    "importlib.reload(utils)\n",
    "\n",
    "config = config_old.Config()\n",
    "image_size = config.image_size[0]\n",
    "anchor_num = len(config.scales) * len(config.ratios) # 一个锚点对应的anchor数量\n",
    "\n",
    "dataset = utils.shapeData(config.image_size, config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(KL.BatchNormalization):\n",
    "    def call(self, inputs, training=None):\n",
    "        return super(self.__class__, self).call(inputs, training=False)\n",
    "\n",
    "# ResNet网络 的building_block\n",
    "# filter：卷积核的通道数\n",
    "# block：block的标识\n",
    "def building_block(filters, block):\n",
    "    if block != 0:\n",
    "        stride = 1\n",
    "    else:\n",
    "        stride = 2\n",
    "    \n",
    "    def f(x):\n",
    "        y = KL.Conv2D(filters, (1,1), strides=stride)(x)\n",
    "        y = BatchNorm(axis=3)(y)\n",
    "        y = KL.Activation(\"relu\")(y)\n",
    "        \n",
    "        y = KL.Conv2D(filters, (3,3), padding=\"same\")(y)\n",
    "        y = BatchNorm(axis=3)(y)\n",
    "        y = KL.Activation(\"relu\")(y)\n",
    "        \n",
    "        y = KL.Conv2D(4 * filters, (1,1))(y)\n",
    "        y = BatchNorm(axis=3)(y)\n",
    "        \n",
    "        if block == 0:\n",
    "            # 保证shorcut的filters和上面y的filters个数一致\n",
    "            shorcut = KL.Conv2D(4 * filters, (1,1), strides=stride)(x)\n",
    "            shorcut = BatchNorm(axis=3)(shorcut)\n",
    "        else:\n",
    "            shorcut = x\n",
    "\n",
    "        # 结合两个支路的输出\n",
    "        y = KL.Add()([y, shorcut])\n",
    "        y = KL.Activation(\"relu\")(y)\n",
    "        return y\n",
    "    return f\n",
    "\n",
    "# ResNet网络\n",
    "def resNet_featureExtractor(inputs):\n",
    "    filters = 64   # 第一个卷积核的通道数\n",
    "    x = KL.Conv2D(filters, (3,3), padding=\"same\")(inputs)\n",
    "    x = BatchNorm(axis=3)(x)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "\n",
    "    # resnet50\n",
    "    blocks = [6, 6, 6]    # buildblock的数量  change\n",
    "    \n",
    "    for i, block_num in enumerate(blocks):\n",
    "        for block_id in range(block_num):\n",
    "            x = building_block(filters, block_id)(x)\n",
    "        filters = filters * 2\n",
    "    return x\n",
    "\n",
    "def rpn_net(inputs, k):\n",
    "    shared_map = KL.Conv2D(256, (3,3), padding=\"same\")(inputs)\n",
    "    shared_map = KL.Activation(\"linear\")(shared_map)\n",
    "    rpn_class = KL.Conv2D(2 * k, (1,1))(shared_map)\n",
    "    rpn_class = KL.Lambda(lambda x: tf.reshape(x, [tf.shape(x)[0], -1, 2]))(rpn_class)\n",
    "    rpn_class = KL.Activation(\"linear\")(rpn_class)\n",
    "    # 分类的得分\n",
    "    rpn_prob = KL.Activation(\"softmax\")(rpn_class)\n",
    "    \n",
    "    y = KL.Conv2D(4*k, (1,1))(shared_map)\n",
    "    y = KL.Activation(\"linear\")(y)\n",
    "    # 边框的得分\n",
    "    rpn_bbox = KL.Lambda(lambda x: tf.reshape(x, [tf.shape(x)[0], -1, 4]))(y)\n",
    "    \n",
    "    return rpn_class, rpn_prob, rpn_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 160, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 160, 160, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_1 (BatchNorm)        (None, 160, 160, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 160, 160, 64) 0           batch_norm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 80, 80, 64)   4160        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_2 (BatchNorm)        (None, 80, 80, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 80, 80, 64)   0           batch_norm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 80, 80, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_3 (BatchNorm)        (None, 80, 80, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 80, 80, 64)   0           batch_norm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 80, 80, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 80, 80, 256)  16640       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_4 (BatchNorm)        (None, 80, 80, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_5 (BatchNorm)        (None, 80, 80, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 80, 80, 256)  0           batch_norm_4[0][0]               \n",
      "                                                                 batch_norm_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 80, 80, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 80, 80, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_6 (BatchNorm)        (None, 80, 80, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 80, 80, 64)   0           batch_norm_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 80, 80, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_7 (BatchNorm)        (None, 80, 80, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 80, 80, 64)   0           batch_norm_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 80, 80, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_8 (BatchNorm)        (None, 80, 80, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 80, 80, 256)  0           batch_norm_8[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 80, 80, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 80, 80, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_9 (BatchNorm)        (None, 80, 80, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 80, 80, 64)   0           batch_norm_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 80, 80, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_10 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 80, 80, 64)   0           batch_norm_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 80, 80, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_11 (BatchNorm)       (None, 80, 80, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 80, 80, 256)  0           batch_norm_11[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 80, 80, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 80, 80, 64)   16448       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_12 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 80, 80, 64)   0           batch_norm_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 80, 80, 64)   36928       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_13 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 80, 80, 64)   0           batch_norm_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 80, 80, 256)  16640       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_14 (BatchNorm)       (None, 80, 80, 256)  1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 80, 80, 256)  0           batch_norm_14[0][0]              \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 80, 80, 256)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 80, 80, 64)   16448       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_15 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 80, 80, 64)   0           batch_norm_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 80, 80, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_16 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 80, 80, 64)   0           batch_norm_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 80, 80, 256)  16640       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_17 (BatchNorm)       (None, 80, 80, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 80, 80, 256)  0           batch_norm_17[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 80, 80, 256)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 80, 80, 64)   16448       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_18 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 80, 80, 64)   0           batch_norm_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 80, 80, 64)   36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_19 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 80, 80, 64)   0           batch_norm_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 80, 80, 256)  16640       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_20 (BatchNorm)       (None, 80, 80, 256)  1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 80, 80, 256)  0           batch_norm_20[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 80, 80, 256)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 40, 40, 128)  32896       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_21 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 40, 40, 128)  0           batch_norm_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 40, 40, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_22 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 40, 40, 128)  0           batch_norm_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 40, 40, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 40, 40, 512)  131584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_23 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_24 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 40, 40, 512)  0           batch_norm_23[0][0]              \n",
      "                                                                 batch_norm_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 40, 40, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 40, 40, 128)  65664       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_25 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 40, 40, 128)  0           batch_norm_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 40, 40, 128)  147584      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_26 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 40, 40, 128)  0           batch_norm_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 40, 40, 512)  66048       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_27 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 40, 40, 512)  0           batch_norm_27[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 40, 40, 512)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 40, 40, 128)  65664       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_28 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 40, 40, 128)  0           batch_norm_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 40, 40, 128)  147584      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_29 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 40, 40, 128)  0           batch_norm_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 40, 40, 512)  66048       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_30 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 40, 40, 512)  0           batch_norm_30[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 40, 40, 512)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 40, 40, 128)  65664       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_31 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 40, 40, 128)  0           batch_norm_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 40, 40, 128)  147584      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_32 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 40, 40, 128)  0           batch_norm_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 40, 40, 512)  66048       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_33 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 40, 40, 512)  0           batch_norm_33[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 40, 40, 512)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 40, 40, 128)  65664       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_34 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 40, 40, 128)  0           batch_norm_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 40, 40, 128)  147584      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_35 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 40, 40, 128)  0           batch_norm_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 40, 40, 512)  66048       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_36 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 40, 40, 512)  0           batch_norm_36[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 40, 40, 512)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 40, 40, 128)  65664       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_37 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 40, 40, 128)  0           batch_norm_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 40, 40, 128)  147584      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_38 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 40, 40, 128)  0           batch_norm_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 40, 40, 512)  66048       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_39 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 40, 40, 512)  0           batch_norm_39[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 40, 40, 512)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 20, 20, 256)  131328      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_40 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 20, 256)  0           batch_norm_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 20, 20, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_41 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 20, 256)  0           batch_norm_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 20, 20, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 20, 20, 1024) 525312      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_42 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_43 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 20, 20, 1024) 0           batch_norm_42[0][0]              \n",
      "                                                                 batch_norm_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 20, 20, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 20, 20, 256)  262400      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_44 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 20, 20, 256)  0           batch_norm_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 20, 20, 256)  590080      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_45 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 20, 20, 256)  0           batch_norm_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 20, 20, 1024) 263168      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_46 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 20, 20, 1024) 0           batch_norm_46[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 20, 20, 1024) 0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 20, 20, 256)  262400      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_47 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 20, 20, 256)  0           batch_norm_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 20, 20, 256)  590080      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_48 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 20, 20, 256)  0           batch_norm_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 20, 20, 1024) 263168      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_49 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 20, 20, 1024) 0           batch_norm_49[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 20, 20, 1024) 0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 20, 20, 256)  262400      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_50 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 20, 20, 256)  0           batch_norm_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 20, 20, 256)  590080      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_51 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 20, 20, 256)  0           batch_norm_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 20, 20, 1024) 263168      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_52 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 20, 20, 1024) 0           batch_norm_52[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 20, 20, 1024) 0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 20, 20, 256)  262400      activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_53 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 20, 20, 256)  0           batch_norm_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 20, 20, 256)  590080      activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_54 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 20, 20, 256)  0           batch_norm_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 20, 20, 1024) 263168      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_55 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 20, 20, 1024) 0           batch_norm_55[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 20, 20, 1024) 0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 20, 20, 256)  262400      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_56 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 20, 20, 256)  0           batch_norm_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 20, 20, 256)  590080      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_57 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 20, 20, 256)  0           batch_norm_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 20, 20, 1024) 263168      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_58 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 20, 20, 1024) 0           batch_norm_58[0][0]              \n",
      "                                                                 activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 20, 20, 1024) 0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 20, 20, 256)  2359552     activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 20, 20, 256)  0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 20, 20, 8)    2056        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 2)      0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 20, 20, 16)   4112        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, 2)      0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 20, 20, 16)   0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, 2)      0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           activation_59[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 11,726,616\n",
      "Trainable params: 11,690,648\n",
      "Non-trainable params: 35,968\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = KL.Input((config.image_size[0], config.image_size[0], 3))  # change\n",
    "fp = resNet_featureExtractor(x)\n",
    "rpn_class, rpn_prob, rpn_bbox = rpn_net(fp, anchor_num)\n",
    "model = Model([x], [rpn_class, rpn_prob, rpn_bbox])\n",
    "model.summary()\n",
    "# plot_model(model, to_file=\"model/model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-ae7db8d23bf6>:15: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x：rpn_box，counts：anchor为1的个数，num_rows = len(counts)：总共多少个\n",
    "# 小函数，取出rpn_box前面有值的“偏移量”\n",
    "def batch_back(x, counts, num_rows):\n",
    "    outputs = []\n",
    "    for i in range(num_rows):\n",
    "        outputs.append(x[i, :counts[i]])\n",
    "    return tf.concat(outputs, axis=0)\n",
    "\n",
    "# 分类loss\n",
    "# rpn_match：真实下标  rpn_class_logits：实际计算出的结果\n",
    "def rpn_class_loss(rpn_match, rpn_class_logits):\n",
    "    # 压缩最后一维，变成一维的坐标，rpn_match (None, 576, 1) => (None, 576) \n",
    "    rpn_match = tf.squeeze(rpn_match, -1)\n",
    "    # 取出 -1 和 1 的坐标\n",
    "    indices = tf.where(K.not_equal(rpn_match, 0))\n",
    "    # 把 -1 和 0 变成 0， 1 还是 1\n",
    "    anchor_class = K.cast(K.equal(rpn_match, 1), tf.int32)\n",
    "    # 取出实际计算出的所有-1 和 1坐标的anchor\n",
    "    rpn_class_logits = tf.gather_nd(rpn_class_logits, indices)     ### prediction\n",
    "    # 取出真实下标的所有-1 和 1坐标的anchor => 现在是（0, 1）0：表示背景 1：表示前景\n",
    "    anchor_class = tf.gather_nd(anchor_class, indices)   ### target\n",
    "    # 计算loss\n",
    "    loss = K.sparse_categorical_crossentropy(target=anchor_class, output=rpn_class_logits, from_logits=True)\n",
    "    # 计算平均，如果loss算出东西了，就取平均，如果没算出东西，就取0\n",
    "    loss = K.switch(tf.size(loss) > 0 , K.mean(loss), tf.constant(0.0))\n",
    "    return loss\n",
    "\n",
    "# 回归loss\n",
    "# target_bbox：真实的边框, rpn_match：真实的anchor下标, rpn_bbox：实际计算出来的结果边框\n",
    "def rpn_bbox_loss(target_bbox, rpn_match, rpn_bbox):\n",
    "    # 压缩最后一维，变成一维的坐标，rpn_match (None, 576, 1) => (None, 576) \n",
    "    rpn_match = tf.squeeze(rpn_match, -1)\n",
    "    # 取出 1 的坐标\n",
    "    indices = tf.where(K.equal(rpn_match, 1))\n",
    "    # 取出所有 1 的“偏移量”\n",
    "    rpn_bbox = tf.gather_nd(rpn_bbox, indices)\n",
    "    # 得出为 1 的“偏移量”的个数\n",
    "    batch_counts = K.sum(K.cast(K.equal(rpn_match, 1), tf.int32), axis=1)\n",
    "    # 取出 target_bbox 中为1的“偏移量”\n",
    "    target_bbox = batch_back(target_bbox, batch_counts, config.batch_size)\n",
    "    # 求误差\n",
    "    diff = K.abs(target_bbox - rpn_bbox)\n",
    "    # 取出小于1的部分\n",
    "    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n",
    "    # 小于1的部分给抛物线  diff**2\n",
    "    # 大于1的部分给直线（不会对大的误差敏感） diff-0.5\n",
    "    loss = (less_than_one * 0.5 * diff**2) + ((1 - less_than_one) * (diff - 0.5))\n",
    "    # 计算平均，如果loss算出东西了，就取平均，如果没算出东西，就取0\n",
    "    loss = K.switch(tf.size(loss) > 0 , K.mean(loss), tf.constant(0.0))\n",
    "    return loss\n",
    "\n",
    "# 图片的输入\n",
    "input_image = KL.Input(shape=[config.image_size[0], config.image_size[0],3], dtype=tf.float32)  # change\n",
    "# 真实的边框输入\n",
    "input_bboxes = KL.Input(shape=[None, 4], dtype=tf.float32)\n",
    "# 真实的分类输入\n",
    "input_class_ids = KL.Input(shape=[None], dtype=tf.int32)\n",
    "# 真实的anchor分类 -1,0,1输入\n",
    "input_rpn_match = KL.Input(shape=[None, 1], dtype=tf.int32)\n",
    "# 真实的anchor偏移量输入\n",
    "input_rpn_bbox = KL.Input(shape=[None, 4], dtype=tf.float32)\n",
    "\n",
    "\n",
    "# 创建模型\n",
    "feature_map = resNet_featureExtractor(input_image)\n",
    "# 模型计算出的结果 rpn_class，rpn_prob，rpn_bbox\n",
    "rpn_class, rpn_prob, rpn_bbox = rpn_net(feature_map, anchor_num)\n",
    "# 分类loss\n",
    "loss_rpn_match = KL.Lambda(lambda x: rpn_class_loss(*x), name=\"loss_rpn_match\")(\n",
    "    [input_rpn_match, rpn_class]\n",
    ")\n",
    "loss_rpn_bbox = KL.Lambda(lambda x:rpn_bbox_loss(*x), name=\"loss_rpn_bbox\")(\n",
    "    [input_rpn_bbox, input_rpn_match, rpn_bbox]\n",
    ")\n",
    "model = Model(\n",
    "    [input_image, input_bboxes, input_class_ids, input_rpn_match, input_rpn_bbox],\n",
    "    [rpn_class, rpn_prob, rpn_bbox, loss_rpn_match, loss_rpn_bbox]\n",
    ")\n",
    "# 添加loss层\n",
    "loss_lay1 = model.get_layer(\"loss_rpn_match\").output\n",
    "loss_lay2 = model.get_layer(\"loss_rpn_bbox\").output\n",
    "model.add_loss(tf.reduce_mean(loss_lay1))\n",
    "model.add_loss(tf.reduce_mean(loss_lay2))\n",
    "# 编译\n",
    "model.compile(loss = [None] * len(model.output), optimizer=keras.optimizers.SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False))\n",
    "# 打印出两个loss的收敛情况\n",
    "model.metrics_names.append(\"loss_rpn_match\")\n",
    "model.metrics_tensors.append(tf.reduce_mean(loss_lay1, keep_dims=True))\n",
    "model.metrics_names.append(\"loss_rpn_bbox\")\n",
    "model.metrics_tensors.append(tf.reduce_mean(loss_lay2, keep_dims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_Gen(num_batch, batch_size, config):\n",
    "    print(\"----------------\")\n",
    "    for iii in range(num_batch):\n",
    "        images = []\n",
    "        bboxes = []\n",
    "        class_ids = []\n",
    "        rpn_matchs = []\n",
    "        rpn_bboxes = []\n",
    "        for i in range(batch_size):\n",
    "            image, bbox, class_id, rpn_match, rpn_bbox, _ = data = dataset.load_data()\n",
    "            pad_num = config.max_gt_obj - bbox.shape[0]\n",
    "            pad_box = np.zeros((pad_num, 4))\n",
    "            pad_ids = np.zeros((pad_num, 1))\n",
    "            bbox = np.concatenate([bbox, pad_box], axis=0)\n",
    "            class_id = np.concatenate([class_id, pad_ids], axis=0)\n",
    "        \n",
    "            images.append(image)\n",
    "            bboxes.append(bbox)\n",
    "            class_ids.append(class_id)\n",
    "            rpn_matchs.append(rpn_match)\n",
    "            rpn_bboxes.append(rpn_bbox)\n",
    "\n",
    "#         print(\"数据：\" + str(index))\n",
    "        images = np.concatenate(images, 0).reshape(batch_size, config.image_size[0],config.image_size[1] , 3)\n",
    "        bboxes = np.concatenate(bboxes, 0).reshape(batch_size, -1 , 4)\n",
    "        class_ids = np.concatenate(class_ids, 0).reshape(batch_size, -1 )\n",
    "        rpn_matchs = np.concatenate(rpn_matchs, 0).reshape(batch_size, -1 , 1)\n",
    "        rpn_bboxes = np.concatenate(rpn_bboxes, 0).reshape(batch_size, -1 , 4)\n",
    "        yield [images, bboxes, class_ids, rpn_matchs, rpn_bboxes],[]\n",
    "\n",
    "total = 200000          # 总数据\n",
    "steps_per_epoch = 20   # 步长\n",
    "dataGen = data_Gen(int(total), config.batch_size, config) # 10000个数据，batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "----------------Epoch 1/200\n",
      "\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 279s 14s/step - loss: 6.2875 - loss_rpn_match: 2.2953 - loss_rpn_bbox: 3.9922\n",
      "Epoch 2/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 294s 15s/step - loss: 0.8813 - loss_rpn_match: 0.4107 - loss_rpn_bbox: 0.4705\n",
      "Epoch 3/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 273s 14s/step - loss: 0.6454 - loss_rpn_match: 0.2753 - loss_rpn_bbox: 0.3702\n",
      "Epoch 4/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 296s 15s/step - loss: 0.6039 - loss_rpn_match: 0.2561 - loss_rpn_bbox: 0.3478\n",
      "Epoch 5/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 290s 15s/step - loss: 0.5749 - loss_rpn_match: 0.2414 - loss_rpn_bbox: 0.3334\n",
      "Epoch 6/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 287s 14s/step - loss: 0.5821 - loss_rpn_match: 0.2436 - loss_rpn_bbox: 0.3385\n",
      "Epoch 7/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 319s 16s/step - loss: 0.5788 - loss_rpn_match: 0.2396 - loss_rpn_bbox: 0.3392\n",
      "Epoch 8/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 305s 15s/step - loss: 0.5753 - loss_rpn_match: 0.2437 - loss_rpn_bbox: 0.3315\n",
      "Epoch 9/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 282s 14s/step - loss: 0.5623 - loss_rpn_match: 0.2341 - loss_rpn_bbox: 0.3283\n",
      "Epoch 10/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5613 - loss_rpn_match: 0.2351 - loss_rpn_bbox: 0.3262\n",
      "Epoch 11/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 253s 13s/step - loss: 0.5445 - loss_rpn_match: 0.2255 - loss_rpn_bbox: 0.3190\n",
      "Epoch 12/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 253s 13s/step - loss: 0.5472 - loss_rpn_match: 0.2296 - loss_rpn_bbox: 0.3176\n",
      "Epoch 13/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5605 - loss_rpn_match: 0.2341 - loss_rpn_bbox: 0.3265\n",
      "Epoch 14/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 253s 13s/step - loss: 0.5531 - loss_rpn_match: 0.2344 - loss_rpn_bbox: 0.3188\n",
      "Epoch 15/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 253s 13s/step - loss: 0.5727 - loss_rpn_match: 0.2445 - loss_rpn_bbox: 0.3282\n",
      "Epoch 16/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5464 - loss_rpn_match: 0.2271 - loss_rpn_bbox: 0.3192\n",
      "Epoch 17/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 254s 13s/step - loss: 0.5346 - loss_rpn_match: 0.2156 - loss_rpn_bbox: 0.3190\n",
      "Epoch 18/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 254s 13s/step - loss: 0.5334 - loss_rpn_match: 0.2193 - loss_rpn_bbox: 0.3140\n",
      "Epoch 19/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5556 - loss_rpn_match: 0.2403 - loss_rpn_bbox: 0.3153\n",
      "Epoch 20/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 254s 13s/step - loss: 0.5495 - loss_rpn_match: 0.2241 - loss_rpn_bbox: 0.3253\n",
      "Epoch 21/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5430 - loss_rpn_match: 0.2290 - loss_rpn_bbox: 0.3141\n",
      "Epoch 22/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5395 - loss_rpn_match: 0.2277 - loss_rpn_bbox: 0.3118\n",
      "Epoch 23/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5418 - loss_rpn_match: 0.2256 - loss_rpn_bbox: 0.3162\n",
      "Epoch 24/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5524 - loss_rpn_match: 0.2279 - loss_rpn_bbox: 0.3245\n",
      "Epoch 25/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5329 - loss_rpn_match: 0.2240 - loss_rpn_bbox: 0.3089\n",
      "Epoch 26/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5314 - loss_rpn_match: 0.2171 - loss_rpn_bbox: 0.3143\n",
      "Epoch 27/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5467 - loss_rpn_match: 0.2278 - loss_rpn_bbox: 0.3189\n",
      "Epoch 28/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5489 - loss_rpn_match: 0.2304 - loss_rpn_bbox: 0.3184\n",
      "Epoch 29/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5485 - loss_rpn_match: 0.2255 - loss_rpn_bbox: 0.3230\n",
      "Epoch 30/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5347 - loss_rpn_match: 0.2212 - loss_rpn_bbox: 0.3135\n",
      "Epoch 31/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5387 - loss_rpn_match: 0.2219 - loss_rpn_bbox: 0.3168\n",
      "Epoch 32/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5407 - loss_rpn_match: 0.2245 - loss_rpn_bbox: 0.3162\n",
      "Epoch 33/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5414 - loss_rpn_match: 0.2267 - loss_rpn_bbox: 0.3148\n",
      "Epoch 34/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5431 - loss_rpn_match: 0.2285 - loss_rpn_bbox: 0.3146\n",
      "Epoch 35/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5308 - loss_rpn_match: 0.2238 - loss_rpn_bbox: 0.3071\n",
      "Epoch 36/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5241 - loss_rpn_match: 0.2149 - loss_rpn_bbox: 0.3092\n",
      "Epoch 37/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5459 - loss_rpn_match: 0.2319 - loss_rpn_bbox: 0.3140\n",
      "Epoch 38/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5186 - loss_rpn_match: 0.2128 - loss_rpn_bbox: 0.3058\n",
      "Epoch 39/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5456 - loss_rpn_match: 0.2305 - loss_rpn_bbox: 0.3151\n",
      "Epoch 40/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5382 - loss_rpn_match: 0.2250 - loss_rpn_bbox: 0.3132\n",
      "Epoch 41/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5360 - loss_rpn_match: 0.2215 - loss_rpn_bbox: 0.3146\n",
      "Epoch 42/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5463 - loss_rpn_match: 0.2325 - loss_rpn_bbox: 0.3138\n",
      "Epoch 43/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5531 - loss_rpn_match: 0.2300 - loss_rpn_bbox: 0.3231\n",
      "Epoch 44/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5341 - loss_rpn_match: 0.2259 - loss_rpn_bbox: 0.3083\n",
      "Epoch 45/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5351 - loss_rpn_match: 0.2222 - loss_rpn_bbox: 0.3129\n",
      "Epoch 46/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5265 - loss_rpn_match: 0.2210 - loss_rpn_bbox: 0.3055\n",
      "Epoch 47/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5321 - loss_rpn_match: 0.2191 - loss_rpn_bbox: 0.3131\n",
      "Epoch 48/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5379 - loss_rpn_match: 0.2265 - loss_rpn_bbox: 0.3114\n",
      "Epoch 49/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5364 - loss_rpn_match: 0.2233 - loss_rpn_bbox: 0.3131\n",
      "Epoch 50/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5402 - loss_rpn_match: 0.2221 - loss_rpn_bbox: 0.3180\n",
      "Epoch 51/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5453 - loss_rpn_match: 0.2280 - loss_rpn_bbox: 0.3172\n",
      "Epoch 52/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5369 - loss_rpn_match: 0.2244 - loss_rpn_bbox: 0.3124\n",
      "Epoch 53/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5350 - loss_rpn_match: 0.2220 - loss_rpn_bbox: 0.3130\n",
      "Epoch 54/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5268 - loss_rpn_match: 0.2199 - loss_rpn_bbox: 0.3070\n",
      "Epoch 55/200\n",
      "学习率：3.125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 261s 13s/step - loss: 0.5352 - loss_rpn_match: 0.2232 - loss_rpn_bbox: 0.3120\n",
      "Epoch 56/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5302 - loss_rpn_match: 0.2193 - loss_rpn_bbox: 0.3109\n",
      "Epoch 57/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5318 - loss_rpn_match: 0.2221 - loss_rpn_bbox: 0.3097\n",
      "Epoch 58/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 263s 13s/step - loss: 0.5244 - loss_rpn_match: 0.2175 - loss_rpn_bbox: 0.3069\n",
      "Epoch 59/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5289 - loss_rpn_match: 0.2184 - loss_rpn_bbox: 0.3106\n",
      "Epoch 60/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5255 - loss_rpn_match: 0.2204 - loss_rpn_bbox: 0.3052\n",
      "Epoch 61/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5445 - loss_rpn_match: 0.2311 - loss_rpn_bbox: 0.3134\n",
      "Epoch 62/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5182 - loss_rpn_match: 0.2151 - loss_rpn_bbox: 0.3031\n",
      "Epoch 63/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5376 - loss_rpn_match: 0.2241 - loss_rpn_bbox: 0.3135\n",
      "Epoch 64/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5350 - loss_rpn_match: 0.2293 - loss_rpn_bbox: 0.3057\n",
      "Epoch 65/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5312 - loss_rpn_match: 0.2230 - loss_rpn_bbox: 0.3082\n",
      "Epoch 66/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5392 - loss_rpn_match: 0.2269 - loss_rpn_bbox: 0.3123\n",
      "Epoch 67/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5385 - loss_rpn_match: 0.2245 - loss_rpn_bbox: 0.3140\n",
      "Epoch 68/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5399 - loss_rpn_match: 0.2224 - loss_rpn_bbox: 0.3175\n",
      "Epoch 69/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5291 - loss_rpn_match: 0.2217 - loss_rpn_bbox: 0.3074\n",
      "Epoch 70/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 263s 13s/step - loss: 0.5344 - loss_rpn_match: 0.2219 - loss_rpn_bbox: 0.3125\n",
      "Epoch 71/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 263s 13s/step - loss: 0.5166 - loss_rpn_match: 0.2121 - loss_rpn_bbox: 0.3044\n",
      "Epoch 72/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 264s 13s/step - loss: 0.5186 - loss_rpn_match: 0.2139 - loss_rpn_bbox: 0.3047\n",
      "Epoch 73/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5378 - loss_rpn_match: 0.2290 - loss_rpn_bbox: 0.3089\n",
      "Epoch 74/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5508 - loss_rpn_match: 0.2384 - loss_rpn_bbox: 0.3124\n",
      "Epoch 75/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5408 - loss_rpn_match: 0.2259 - loss_rpn_bbox: 0.3149\n",
      "Epoch 76/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5296 - loss_rpn_match: 0.2194 - loss_rpn_bbox: 0.3102\n",
      "Epoch 77/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5352 - loss_rpn_match: 0.2230 - loss_rpn_bbox: 0.3123\n",
      "Epoch 78/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5585 - loss_rpn_match: 0.2360 - loss_rpn_bbox: 0.3225\n",
      "Epoch 79/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5331 - loss_rpn_match: 0.2249 - loss_rpn_bbox: 0.3082\n",
      "Epoch 80/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5498 - loss_rpn_match: 0.2306 - loss_rpn_bbox: 0.3192\n",
      "Epoch 81/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5305 - loss_rpn_match: 0.2194 - loss_rpn_bbox: 0.3111\n",
      "Epoch 82/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5246 - loss_rpn_match: 0.2154 - loss_rpn_bbox: 0.3091\n",
      "Epoch 83/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5404 - loss_rpn_match: 0.2229 - loss_rpn_bbox: 0.3175\n",
      "Epoch 84/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5306 - loss_rpn_match: 0.2171 - loss_rpn_bbox: 0.3135\n",
      "Epoch 85/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5354 - loss_rpn_match: 0.2231 - loss_rpn_bbox: 0.3123\n",
      "Epoch 86/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5414 - loss_rpn_match: 0.2313 - loss_rpn_bbox: 0.3102\n",
      "Epoch 87/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5382 - loss_rpn_match: 0.2194 - loss_rpn_bbox: 0.3188\n",
      "Epoch 88/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5245 - loss_rpn_match: 0.2161 - loss_rpn_bbox: 0.3084\n",
      "Epoch 89/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5379 - loss_rpn_match: 0.2256 - loss_rpn_bbox: 0.3124\n",
      "Epoch 90/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5262 - loss_rpn_match: 0.2193 - loss_rpn_bbox: 0.3069\n",
      "Epoch 91/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5389 - loss_rpn_match: 0.2299 - loss_rpn_bbox: 0.3089\n",
      "Epoch 92/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5410 - loss_rpn_match: 0.2290 - loss_rpn_bbox: 0.3121\n",
      "Epoch 93/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5323 - loss_rpn_match: 0.2254 - loss_rpn_bbox: 0.3068\n",
      "Epoch 94/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5198 - loss_rpn_match: 0.2210 - loss_rpn_bbox: 0.2988\n",
      "Epoch 95/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5334 - loss_rpn_match: 0.2243 - loss_rpn_bbox: 0.3092\n",
      "Epoch 96/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5157 - loss_rpn_match: 0.2121 - loss_rpn_bbox: 0.3037\n",
      "Epoch 97/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5274 - loss_rpn_match: 0.2202 - loss_rpn_bbox: 0.3072\n",
      "Epoch 98/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5371 - loss_rpn_match: 0.2267 - loss_rpn_bbox: 0.3105\n",
      "Epoch 99/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5290 - loss_rpn_match: 0.2184 - loss_rpn_bbox: 0.3106\n",
      "Epoch 100/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5415 - loss_rpn_match: 0.2317 - loss_rpn_bbox: 0.3098\n",
      "Epoch 101/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5473 - loss_rpn_match: 0.2293 - loss_rpn_bbox: 0.3180\n",
      "Epoch 102/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5364 - loss_rpn_match: 0.2228 - loss_rpn_bbox: 0.3137\n",
      "Epoch 103/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5337 - loss_rpn_match: 0.2296 - loss_rpn_bbox: 0.3041\n",
      "Epoch 104/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5355 - loss_rpn_match: 0.2187 - loss_rpn_bbox: 0.3167\n",
      "Epoch 105/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5476 - loss_rpn_match: 0.2333 - loss_rpn_bbox: 0.3143\n",
      "Epoch 106/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5318 - loss_rpn_match: 0.2168 - loss_rpn_bbox: 0.3150\n",
      "Epoch 107/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5291 - loss_rpn_match: 0.2178 - loss_rpn_bbox: 0.3112\n",
      "Epoch 108/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5338 - loss_rpn_match: 0.2195 - loss_rpn_bbox: 0.3143\n",
      "Epoch 109/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5308 - loss_rpn_match: 0.2242 - loss_rpn_bbox: 0.3065\n",
      "Epoch 110/200\n",
      "学习率：4.8828125e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 259s 13s/step - loss: 0.5346 - loss_rpn_match: 0.2234 - loss_rpn_bbox: 0.3113\n",
      "Epoch 111/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5302 - loss_rpn_match: 0.2233 - loss_rpn_bbox: 0.3069\n",
      "Epoch 112/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5322 - loss_rpn_match: 0.2230 - loss_rpn_bbox: 0.3093\n",
      "Epoch 113/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5388 - loss_rpn_match: 0.2302 - loss_rpn_bbox: 0.3086\n",
      "Epoch 114/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5363 - loss_rpn_match: 0.2234 - loss_rpn_bbox: 0.3129\n",
      "Epoch 115/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5409 - loss_rpn_match: 0.2234 - loss_rpn_bbox: 0.3175\n",
      "Epoch 116/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5337 - loss_rpn_match: 0.2227 - loss_rpn_bbox: 0.3110\n",
      "Epoch 117/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5279 - loss_rpn_match: 0.2202 - loss_rpn_bbox: 0.3077\n",
      "Epoch 118/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5289 - loss_rpn_match: 0.2193 - loss_rpn_bbox: 0.3096\n",
      "Epoch 119/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5264 - loss_rpn_match: 0.2202 - loss_rpn_bbox: 0.3062\n",
      "Epoch 120/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5491 - loss_rpn_match: 0.2316 - loss_rpn_bbox: 0.3174\n",
      "Epoch 121/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5408 - loss_rpn_match: 0.2256 - loss_rpn_bbox: 0.3152\n",
      "Epoch 122/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5152 - loss_rpn_match: 0.2105 - loss_rpn_bbox: 0.3047\n",
      "Epoch 123/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5302 - loss_rpn_match: 0.2196 - loss_rpn_bbox: 0.3106\n",
      "Epoch 124/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5338 - loss_rpn_match: 0.2275 - loss_rpn_bbox: 0.3063\n",
      "Epoch 125/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5389 - loss_rpn_match: 0.2245 - loss_rpn_bbox: 0.3144\n",
      "Epoch 126/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5346 - loss_rpn_match: 0.2264 - loss_rpn_bbox: 0.3081\n",
      "Epoch 127/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5317 - loss_rpn_match: 0.2215 - loss_rpn_bbox: 0.3102\n",
      "Epoch 128/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5454 - loss_rpn_match: 0.2285 - loss_rpn_bbox: 0.3169\n",
      "Epoch 129/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5374 - loss_rpn_match: 0.2243 - loss_rpn_bbox: 0.3131\n",
      "Epoch 130/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5177 - loss_rpn_match: 0.2151 - loss_rpn_bbox: 0.3026\n",
      "Epoch 131/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5222 - loss_rpn_match: 0.2107 - loss_rpn_bbox: 0.3115\n",
      "Epoch 132/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5349 - loss_rpn_match: 0.2247 - loss_rpn_bbox: 0.3102\n",
      "Epoch 133/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5172 - loss_rpn_match: 0.2143 - loss_rpn_bbox: 0.3029\n",
      "Epoch 134/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5286 - loss_rpn_match: 0.2193 - loss_rpn_bbox: 0.3093\n",
      "Epoch 135/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5418 - loss_rpn_match: 0.2251 - loss_rpn_bbox: 0.3167\n",
      "Epoch 136/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5397 - loss_rpn_match: 0.2230 - loss_rpn_bbox: 0.3167\n",
      "Epoch 137/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5350 - loss_rpn_match: 0.2251 - loss_rpn_bbox: 0.3098\n",
      "Epoch 138/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5407 - loss_rpn_match: 0.2307 - loss_rpn_bbox: 0.3100\n",
      "Epoch 139/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5282 - loss_rpn_match: 0.2211 - loss_rpn_bbox: 0.3071\n",
      "Epoch 140/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5274 - loss_rpn_match: 0.2177 - loss_rpn_bbox: 0.3096\n",
      "Epoch 141/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5387 - loss_rpn_match: 0.2280 - loss_rpn_bbox: 0.3107\n",
      "Epoch 142/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5432 - loss_rpn_match: 0.2302 - loss_rpn_bbox: 0.3130\n",
      "Epoch 143/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5364 - loss_rpn_match: 0.2300 - loss_rpn_bbox: 0.3065\n",
      "Epoch 144/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5171 - loss_rpn_match: 0.2171 - loss_rpn_bbox: 0.3000\n",
      "Epoch 145/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5414 - loss_rpn_match: 0.2267 - loss_rpn_bbox: 0.3148\n",
      "Epoch 146/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5387 - loss_rpn_match: 0.2242 - loss_rpn_bbox: 0.3145\n",
      "Epoch 147/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5328 - loss_rpn_match: 0.2225 - loss_rpn_bbox: 0.3103\n",
      "Epoch 148/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5246 - loss_rpn_match: 0.2194 - loss_rpn_bbox: 0.3052\n",
      "Epoch 149/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5298 - loss_rpn_match: 0.2241 - loss_rpn_bbox: 0.3057\n",
      "Epoch 150/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5313 - loss_rpn_match: 0.2236 - loss_rpn_bbox: 0.3077\n",
      "Epoch 151/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5441 - loss_rpn_match: 0.2290 - loss_rpn_bbox: 0.3151\n",
      "Epoch 152/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5309 - loss_rpn_match: 0.2204 - loss_rpn_bbox: 0.3106\n",
      "Epoch 153/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5437 - loss_rpn_match: 0.2333 - loss_rpn_bbox: 0.3104\n",
      "Epoch 154/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5183 - loss_rpn_match: 0.2134 - loss_rpn_bbox: 0.3049\n",
      "Epoch 155/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5382 - loss_rpn_match: 0.2235 - loss_rpn_bbox: 0.3148\n",
      "Epoch 156/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5319 - loss_rpn_match: 0.2206 - loss_rpn_bbox: 0.3113\n",
      "Epoch 157/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5295 - loss_rpn_match: 0.2224 - loss_rpn_bbox: 0.3071\n",
      "Epoch 158/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5359 - loss_rpn_match: 0.2236 - loss_rpn_bbox: 0.3124\n",
      "Epoch 159/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5307 - loss_rpn_match: 0.2234 - loss_rpn_bbox: 0.3072\n",
      "Epoch 160/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5368 - loss_rpn_match: 0.2301 - loss_rpn_bbox: 0.3067\n",
      "Epoch 161/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5560 - loss_rpn_match: 0.2376 - loss_rpn_bbox: 0.3185\n",
      "Epoch 162/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5264 - loss_rpn_match: 0.2199 - loss_rpn_bbox: 0.3065\n",
      "Epoch 163/200\n",
      "学习率：1.52587890625e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 259s 13s/step - loss: 0.5334 - loss_rpn_match: 0.2216 - loss_rpn_bbox: 0.3118\n",
      "Epoch 164/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5363 - loss_rpn_match: 0.2254 - loss_rpn_bbox: 0.3109\n",
      "Epoch 165/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5261 - loss_rpn_match: 0.2169 - loss_rpn_bbox: 0.3092\n",
      "Epoch 166/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5358 - loss_rpn_match: 0.2217 - loss_rpn_bbox: 0.3141\n",
      "Epoch 167/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5281 - loss_rpn_match: 0.2202 - loss_rpn_bbox: 0.3079\n",
      "Epoch 168/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5229 - loss_rpn_match: 0.2165 - loss_rpn_bbox: 0.3064\n",
      "Epoch 169/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5323 - loss_rpn_match: 0.2236 - loss_rpn_bbox: 0.3087\n",
      "Epoch 170/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5507 - loss_rpn_match: 0.2333 - loss_rpn_bbox: 0.3174\n",
      "Epoch 171/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5446 - loss_rpn_match: 0.2285 - loss_rpn_bbox: 0.3161\n",
      "Epoch 172/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5401 - loss_rpn_match: 0.2288 - loss_rpn_bbox: 0.3112\n",
      "Epoch 173/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5476 - loss_rpn_match: 0.2283 - loss_rpn_bbox: 0.3193\n",
      "Epoch 174/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5319 - loss_rpn_match: 0.2217 - loss_rpn_bbox: 0.3103\n",
      "Epoch 175/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5416 - loss_rpn_match: 0.2203 - loss_rpn_bbox: 0.3212\n",
      "Epoch 176/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5306 - loss_rpn_match: 0.2279 - loss_rpn_bbox: 0.3027\n",
      "Epoch 177/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5369 - loss_rpn_match: 0.2201 - loss_rpn_bbox: 0.3167\n",
      "Epoch 178/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5305 - loss_rpn_match: 0.2192 - loss_rpn_bbox: 0.3113\n",
      "Epoch 179/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5334 - loss_rpn_match: 0.2215 - loss_rpn_bbox: 0.3119\n",
      "Epoch 180/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5366 - loss_rpn_match: 0.2265 - loss_rpn_bbox: 0.3101\n",
      "Epoch 181/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5265 - loss_rpn_match: 0.2212 - loss_rpn_bbox: 0.3053\n",
      "Epoch 182/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5327 - loss_rpn_match: 0.2259 - loss_rpn_bbox: 0.3067\n",
      "Epoch 183/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5338 - loss_rpn_match: 0.2197 - loss_rpn_bbox: 0.3141\n",
      "Epoch 184/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5327 - loss_rpn_match: 0.2229 - loss_rpn_bbox: 0.3098\n",
      "Epoch 185/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5418 - loss_rpn_match: 0.2273 - loss_rpn_bbox: 0.3144\n",
      "Epoch 186/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5428 - loss_rpn_match: 0.2241 - loss_rpn_bbox: 0.3187\n",
      "Epoch 187/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5460 - loss_rpn_match: 0.2277 - loss_rpn_bbox: 0.3182\n",
      "Epoch 188/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5339 - loss_rpn_match: 0.2213 - loss_rpn_bbox: 0.3127\n",
      "Epoch 189/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5318 - loss_rpn_match: 0.2216 - loss_rpn_bbox: 0.3102\n",
      "Epoch 190/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5419 - loss_rpn_match: 0.2314 - loss_rpn_bbox: 0.3105\n",
      "Epoch 191/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5348 - loss_rpn_match: 0.2245 - loss_rpn_bbox: 0.3103\n",
      "Epoch 192/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5260 - loss_rpn_match: 0.2210 - loss_rpn_bbox: 0.3050\n",
      "Epoch 193/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5336 - loss_rpn_match: 0.2217 - loss_rpn_bbox: 0.3119\n",
      "Epoch 194/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5310 - loss_rpn_match: 0.2223 - loss_rpn_bbox: 0.3087\n",
      "Epoch 195/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5323 - loss_rpn_match: 0.2200 - loss_rpn_bbox: 0.3124\n",
      "Epoch 196/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5274 - loss_rpn_match: 0.2238 - loss_rpn_bbox: 0.3036\n",
      "Epoch 197/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5329 - loss_rpn_match: 0.2212 - loss_rpn_bbox: 0.3116\n",
      "Epoch 198/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5320 - loss_rpn_match: 0.2185 - loss_rpn_bbox: 0.3135\n",
      "Epoch 199/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5336 - loss_rpn_match: 0.2199 - loss_rpn_bbox: 0.3137\n",
      "Epoch 200/200\n",
      "学习率：9.5367431640625e-11\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5299 - loss_rpn_match: 0.2219 - loss_rpn_bbox: 0.3080\n"
     ]
    }
   ],
   "source": [
    "# face V2版本\n",
    "# 参数1320万 320 × 320  batch_size：10 steps_per_epoch：20  rpn_stride：16  scales：[55, 60, 70, 85, 105, 140, 180, 230]  buildblock：[7,7,7]   文件名：model_224_16_[epochs数]_v2.h5\n",
    "# 第一次20epochs lr=0.0001 batch_size：10  1w数据\n",
    "# drop = 0.5 衰减率调小\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    print('学习率：' + str(lrate))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "his = model.fit_generator(dataGen, steps_per_epoch=steps_per_epoch, epochs=200, callbacks=[lrate])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"model_material_64_200.h5\")\n",
    "model.load_weights(\"model_material_64_200.h5\")\n",
    "# model.save_weights(\"model_material_160_200.h5\")\n",
    "# model.load_weights(\"model_material1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将anchors根据计算出的修正量逆运算出实际计算的anchors\n",
    "# anchors：锚框\n",
    "# deltas：网络计算出的修正量\n",
    "def anchor_refinement(anchors, deltas):\n",
    "    boxes = tf.cast(anchors, tf.float32)\n",
    "    h = boxes[:, 2] - boxes[:, 0]\n",
    "    w = boxes[:, 3] - boxes[:, 1]\n",
    "    center_y = boxes[:, 0] + h / 2\n",
    "    center_x = boxes[:, 1] + w / 2\n",
    "\n",
    "    center_y += deltas[:, 0] * h\n",
    "    center_x += deltas[:, 1] * w\n",
    "    h *= tf.exp(deltas[:, 2])\n",
    "    w *= tf.exp(deltas[:, 3])\n",
    "    \n",
    "    y1 = center_y - h / 2\n",
    "    x1 = center_x - w / 2\n",
    "    y2 = center_y + h / 2\n",
    "    x2 = center_x + w / 2\n",
    "    boxes = tf.stack([y1, x1, y2, x2], axis=1)\n",
    "    return boxes\n",
    "\n",
    "# 限定坐标的边界，不超过边界\n",
    "def boxes_clip(boxes, window):\n",
    "    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n",
    "    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n",
    "    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n",
    "    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n",
    "    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n",
    "    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n",
    "    cliped = tf.concat([y1, x1, y2, x2], axis=1)\n",
    "    cliped.set_shape((cliped.shape[0], 4))\n",
    "    return cliped\n",
    "\n",
    "# 将输入的inputs 按提供的 graph_fn方法进行切片\n",
    "# 因为数据是batch_size个的数组，所以这个函数是进行batch_size个数组的操作\n",
    "def batch_slice(inputs, graph_fn, batch_size):\n",
    "    if not isinstance(inputs, list):\n",
    "        inputs = [inputs]\n",
    "    output = []\n",
    "    for i in range(batch_size):\n",
    "        inputs_slice = [x[i] for x in inputs]\n",
    "        output_slice = graph_fn(*inputs_slice)\n",
    "        if not isinstance(output_slice, (list, tuple)):\n",
    "            output_slice = [output_slice]\n",
    "        output.append(output_slice)\n",
    "    # 打包输出出去\n",
    "    output = list(zip(*output))\n",
    "    result = [tf.stack(o, axis=0) for o in output]\n",
    "    if len(result)==1:\n",
    "        result = result[0]\n",
    "    return result\n",
    "\n",
    "\n",
    "import keras.engine as KE\n",
    "class proposal(KE.Layer):\n",
    "    # proposal_count：保留的框个数\n",
    "    # nms_thresh：超过这个阈值，两个anchor就去pk\n",
    "    # anchors：锚框\n",
    "    # batch_size\n",
    "    # config：配置项\n",
    "    # kwargs：其他参数\n",
    "    def __init__(self, proposal_count, nms_thresh, anchors, batch_size, config=None, **kwargs):\n",
    "        super(proposal, self).__init__(**kwargs)\n",
    "        self.proposal_count = proposal_count\n",
    "        self.anchors = anchors\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.batch_size = batch_size\n",
    "        self.config = config\n",
    "    \n",
    "    # input：输入的 probs置信度inputs[0] 和 deltas修正量inputs[1]\n",
    "    def call(self, inputs):\n",
    "        # 取最后一维（前景概率）\n",
    "        probs = inputs[0][:, :, 1]\n",
    "        # 修正量\n",
    "        deltas = inputs[1]\n",
    "        # -------------------------------- 取得分前100的锚框数据 --------------------------------\n",
    "        # 乘RPN_BBOX_STD_DEV，是因为build_rpnTarget函数里面除以了这个数，避免值太小\n",
    "        deltas = deltas * np.reshape(self.config.RPN_BBOX_STD_DEV, (1, 1, 4))\n",
    "        # 判断：取100和anchors数之间最小\n",
    "        prenms_num = min(100, self.anchors.shape[0])\n",
    "        # 取前100个锚框得分最高的锚框下标\n",
    "        idxs = tf.nn.top_k(probs, prenms_num).indices\n",
    "        \n",
    "        # 按坐标取元素\n",
    "        # 置信度\n",
    "        probs = batch_slice([probs, idxs], lambda x,y:tf.gather(x, y), self.batch_size)\n",
    "        # 修正量\n",
    "        deltas = batch_slice([deltas, idxs], lambda x,y:tf.gather(x, y), self.batch_size)\n",
    "        # 锚框\n",
    "        anchors = batch_slice([idxs], lambda x:tf.gather(self.anchors,x), self.batch_size)\n",
    "\n",
    "        # ----------------------------------------- 修正 -----------------------------------------\n",
    "        # 取修正框\n",
    "        refined_boxes = batch_slice([anchors, deltas], lambda x,y:anchor_refinement(x,y), self.batch_size)\n",
    "        # 限定坐标的边界，不超过边界，避免修到外面去\n",
    "        H,W = self.config.image_size[:2]\n",
    "        windows = np.array([0,0,H,W]).astype(np.float32)\n",
    "        cliped_boxes = batch_slice([refined_boxes], lambda x:boxes_clip(x, windows), self.batch_size)\n",
    "        \n",
    "        # 归一化\n",
    "        normalized_boxes = cliped_boxes / np.array([H,W,H,W])\n",
    "        # 非极大值抑制方法\n",
    "        def nms(normalized_boxes, scores):\n",
    "            idxs_ = tf.image.non_max_suppression(normalized_boxes, scores, self.proposal_count, self.nms_thresh)\n",
    "            box = tf.gather(normalized_boxes, idxs_)\n",
    "            pad_num = tf.maximum(self.proposal_count - tf.shape(normalized_boxes)[0],0)\n",
    "            box = tf.pad(box, [(0,pad_num),(0,0)])\n",
    "            return box\n",
    "        # 进行非极大值抑制切换\n",
    "        proposal_ = batch_slice([normalized_boxes, probs], nms, self.batch_size)\n",
    "        return proposal_\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.proposal_count, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 160, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "test_data = next(dataGen)[0]\n",
    "images = test_data[0]\n",
    "bboxes = test_data[1]\n",
    "class_ids = test_data[2]\n",
    "rpn_matchs = test_data[3]\n",
    "rpn_bboxes = test_data[4]\n",
    "print(images.shape)\n",
    "rpn_class, rpn_prob, rpn_bbox, _, _ = \\\n",
    "                model.predict([images, bboxes, class_ids, rpn_matchs, rpn_bboxes])\n",
    "# 转tensor\n",
    "rpn_class = tf.convert_to_tensor(rpn_class)\n",
    "rpn_prob = tf.convert_to_tensor(rpn_prob)\n",
    "rpn_bbox = tf.convert_to_tensor(rpn_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "# change\n",
    "anchors = utils.anchor_gen(config.featureMap_size, ratios=config.ratios, scales=config.scales, rpn_stride=config.rpn_stride,\n",
    "                           anchor_stride = config.anchor_stride)\n",
    "proposals = proposal(proposal_count=15, nms_thresh=0.7, anchors=anchors, batch_size=config.batch_size, config=config)([rpn_prob, rpn_bbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "proposals_ = sess.run(proposals) * image_size # change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ix = 2\n",
    "ix = random.sample(range(config.batch_size), 1)[0]\n",
    "proposal_ = proposals_[ix]\n",
    "img = images[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[105.64445   42.221474 132.68839   69.55856 ]\n",
      " [127.75143   81.25125  160.       121.33006 ]\n",
      " [ 96.819275   0.       131.33371   24.137833]\n",
      " [ 98.44667  137.72272  126.6188   160.      ]\n",
      " [ 28.762024  30.214245  65.68754   55.227962]\n",
      " [  0.        79.43198   38.71447  123.75572 ]\n",
      " [107.359344   0.       132.8097    20.784662]\n",
      " [ 66.063095   0.        96.31372   13.762424]\n",
      " [ 75.17852   42.953793 100.220375  69.56235 ]\n",
      " [ 75.50048    0.       101.10702   19.90926 ]\n",
      " [ 57.522236  97.524155  89.08665  141.0845  ]\n",
      " [ 30.22485   94.992325  62.219563 124.66997 ]\n",
      " [ 43.743103  42.38432   66.13461   68.50126 ]\n",
      " [ 99.238365  97.1914   126.75149  125.10352 ]\n",
      " [107.383514  60.750168 134.11389   86.09253 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(img)\n",
    "axs = plt.gca()\n",
    "print(proposal_)\n",
    "for i in range(6):\n",
    "    box = proposal_[i]\n",
    "    rec = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], facecolor='none', edgecolor='r')\n",
    "    axs.add_patch(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
