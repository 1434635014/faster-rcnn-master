{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.layers as KL\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import importlib\n",
    "import config_old\n",
    "import math\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import utils\n",
    "\n",
    "importlib.reload(config_old)\n",
    "importlib.reload(utils)\n",
    "\n",
    "config = config_old.Config()\n",
    "image_size = config.image_size[0]\n",
    "anchor_num = len(config.scales) * len(config.ratios) # 一个锚点对应的anchor数量\n",
    "\n",
    "dataset = utils.shapeData(config.image_size, config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(KL.BatchNormalization):\n",
    "    def call(self, inputs, training=None):\n",
    "        return super(self.__class__, self).call(inputs, training=False)\n",
    "\n",
    "# ResNet网络 的building_block\n",
    "# filter：卷积核的通道数\n",
    "# block：block的标识\n",
    "def building_block(filters, block):\n",
    "    if block != 0:\n",
    "        stride = 1\n",
    "    else:\n",
    "        stride = 2\n",
    "    \n",
    "    def f(x):\n",
    "        y = KL.Conv2D(filters, (1,1), strides=stride)(x)\n",
    "        y = BatchNorm(axis=3)(y)\n",
    "        y = KL.Activation(\"relu\")(y)\n",
    "        \n",
    "        y = KL.Conv2D(filters, (3,3), padding=\"same\")(y)\n",
    "        y = BatchNorm(axis=3)(y)\n",
    "        y = KL.Activation(\"relu\")(y)\n",
    "        \n",
    "        y = KL.Conv2D(4 * filters, (1,1))(y)\n",
    "        y = BatchNorm(axis=3)(y)\n",
    "        \n",
    "        if block == 0:\n",
    "            # 保证shorcut的filters和上面y的filters个数一致\n",
    "            shorcut = KL.Conv2D(4 * filters, (1,1), strides=stride)(x)\n",
    "            shorcut = BatchNorm(axis=3)(shorcut)\n",
    "        else:\n",
    "            shorcut = x\n",
    "\n",
    "        # 结合两个支路的输出\n",
    "        y = KL.Add()([y, shorcut])\n",
    "        y = KL.Activation(\"relu\")(y)\n",
    "        return y\n",
    "    return f\n",
    "\n",
    "# ResNet网络\n",
    "def resNet_featureExtractor(inputs):\n",
    "    filters = 64   # 第一个卷积核的通道数\n",
    "    x = KL.Conv2D(filters, (3,3), padding=\"same\")(inputs)\n",
    "    x = BatchNorm(axis=3)(x)\n",
    "    x = KL.Activation(\"relu\")(x)\n",
    "\n",
    "    # resnet50\n",
    "    blocks = [6, 6, 6]    # buildblock的数量  change\n",
    "    \n",
    "    for i, block_num in enumerate(blocks):\n",
    "        for block_id in range(block_num):\n",
    "            x = building_block(filters, block_id)(x)\n",
    "        filters = filters * 2\n",
    "    return x\n",
    "\n",
    "def rpn_net(inputs, k):\n",
    "    shared_map = KL.Conv2D(256, (3,3), padding=\"same\")(inputs)\n",
    "    shared_map = KL.Activation(\"linear\")(shared_map)\n",
    "    rpn_class = KL.Conv2D(2 * k, (1,1))(shared_map)\n",
    "    rpn_class = KL.Lambda(lambda x: tf.reshape(x, [tf.shape(x)[0], -1, 2]))(rpn_class)\n",
    "    rpn_class = KL.Activation(\"linear\")(rpn_class)\n",
    "    # 分类的得分\n",
    "    rpn_prob = KL.Activation(\"softmax\")(rpn_class)\n",
    "    \n",
    "    y = KL.Conv2D(4*k, (1,1))(shared_map)\n",
    "    y = KL.Activation(\"linear\")(y)\n",
    "    # 边框的得分\n",
    "    rpn_bbox = KL.Lambda(lambda x: tf.reshape(x, [tf.shape(x)[0], -1, 4]))(y)\n",
    "    \n",
    "    return rpn_class, rpn_prob, rpn_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 160, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 160, 160, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_1 (BatchNorm)        (None, 160, 160, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 160, 160, 64) 0           batch_norm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 80, 80, 64)   4160        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_2 (BatchNorm)        (None, 80, 80, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 80, 80, 64)   0           batch_norm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 80, 80, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_3 (BatchNorm)        (None, 80, 80, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 80, 80, 64)   0           batch_norm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 80, 80, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 80, 80, 256)  16640       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_4 (BatchNorm)        (None, 80, 80, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_5 (BatchNorm)        (None, 80, 80, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 80, 80, 256)  0           batch_norm_4[0][0]               \n",
      "                                                                 batch_norm_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 80, 80, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 80, 80, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_6 (BatchNorm)        (None, 80, 80, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 80, 80, 64)   0           batch_norm_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 80, 80, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_7 (BatchNorm)        (None, 80, 80, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 80, 80, 64)   0           batch_norm_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 80, 80, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_8 (BatchNorm)        (None, 80, 80, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 80, 80, 256)  0           batch_norm_8[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 80, 80, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 80, 80, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_9 (BatchNorm)        (None, 80, 80, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 80, 80, 64)   0           batch_norm_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 80, 80, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_10 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 80, 80, 64)   0           batch_norm_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 80, 80, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_11 (BatchNorm)       (None, 80, 80, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 80, 80, 256)  0           batch_norm_11[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 80, 80, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 80, 80, 64)   16448       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_12 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 80, 80, 64)   0           batch_norm_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 80, 80, 64)   36928       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_13 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 80, 80, 64)   0           batch_norm_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 80, 80, 256)  16640       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_14 (BatchNorm)       (None, 80, 80, 256)  1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 80, 80, 256)  0           batch_norm_14[0][0]              \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 80, 80, 256)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 80, 80, 64)   16448       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_15 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 80, 80, 64)   0           batch_norm_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 80, 80, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_16 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 80, 80, 64)   0           batch_norm_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 80, 80, 256)  16640       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_17 (BatchNorm)       (None, 80, 80, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 80, 80, 256)  0           batch_norm_17[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 80, 80, 256)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 80, 80, 64)   16448       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_18 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 80, 80, 64)   0           batch_norm_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 80, 80, 64)   36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_19 (BatchNorm)       (None, 80, 80, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 80, 80, 64)   0           batch_norm_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 80, 80, 256)  16640       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_20 (BatchNorm)       (None, 80, 80, 256)  1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 80, 80, 256)  0           batch_norm_20[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 80, 80, 256)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 40, 40, 128)  32896       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_21 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 40, 40, 128)  0           batch_norm_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 40, 40, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_22 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 40, 40, 128)  0           batch_norm_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 40, 40, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 40, 40, 512)  131584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_23 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_24 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 40, 40, 512)  0           batch_norm_23[0][0]              \n",
      "                                                                 batch_norm_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 40, 40, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 40, 40, 128)  65664       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_25 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 40, 40, 128)  0           batch_norm_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 40, 40, 128)  147584      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_26 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 40, 40, 128)  0           batch_norm_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 40, 40, 512)  66048       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_27 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 40, 40, 512)  0           batch_norm_27[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 40, 40, 512)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 40, 40, 128)  65664       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_28 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 40, 40, 128)  0           batch_norm_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 40, 40, 128)  147584      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_29 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 40, 40, 128)  0           batch_norm_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 40, 40, 512)  66048       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_30 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 40, 40, 512)  0           batch_norm_30[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 40, 40, 512)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 40, 40, 128)  65664       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_31 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 40, 40, 128)  0           batch_norm_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 40, 40, 128)  147584      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_32 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 40, 40, 128)  0           batch_norm_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 40, 40, 512)  66048       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_33 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 40, 40, 512)  0           batch_norm_33[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 40, 40, 512)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 40, 40, 128)  65664       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_34 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 40, 40, 128)  0           batch_norm_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 40, 40, 128)  147584      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_35 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 40, 40, 128)  0           batch_norm_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 40, 40, 512)  66048       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_36 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 40, 40, 512)  0           batch_norm_36[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 40, 40, 512)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 40, 40, 128)  65664       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_37 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 40, 40, 128)  0           batch_norm_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 40, 40, 128)  147584      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_38 (BatchNorm)       (None, 40, 40, 128)  512         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 40, 40, 128)  0           batch_norm_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 40, 40, 512)  66048       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_39 (BatchNorm)       (None, 40, 40, 512)  2048        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 40, 40, 512)  0           batch_norm_39[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 40, 40, 512)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 20, 20, 256)  131328      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_40 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 20, 256)  0           batch_norm_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 20, 20, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_41 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 20, 256)  0           batch_norm_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 20, 20, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 20, 20, 1024) 525312      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_42 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_43 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 20, 20, 1024) 0           batch_norm_42[0][0]              \n",
      "                                                                 batch_norm_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 20, 20, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 20, 20, 256)  262400      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_44 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 20, 20, 256)  0           batch_norm_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 20, 20, 256)  590080      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_45 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 20, 20, 256)  0           batch_norm_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 20, 20, 1024) 263168      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_46 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 20, 20, 1024) 0           batch_norm_46[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 20, 20, 1024) 0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 20, 20, 256)  262400      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_47 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 20, 20, 256)  0           batch_norm_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 20, 20, 256)  590080      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_48 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 20, 20, 256)  0           batch_norm_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 20, 20, 1024) 263168      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_49 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 20, 20, 1024) 0           batch_norm_49[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 20, 20, 1024) 0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 20, 20, 256)  262400      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_50 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 20, 20, 256)  0           batch_norm_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 20, 20, 256)  590080      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_51 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 20, 20, 256)  0           batch_norm_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 20, 20, 1024) 263168      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_52 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 20, 20, 1024) 0           batch_norm_52[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 20, 20, 1024) 0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 20, 20, 256)  262400      activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_53 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 20, 20, 256)  0           batch_norm_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 20, 20, 256)  590080      activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_54 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 20, 20, 256)  0           batch_norm_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 20, 20, 1024) 263168      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_55 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 20, 20, 1024) 0           batch_norm_55[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 20, 20, 1024) 0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 20, 20, 256)  262400      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_56 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 20, 20, 256)  0           batch_norm_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 20, 20, 256)  590080      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_57 (BatchNorm)       (None, 20, 20, 256)  1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 20, 20, 256)  0           batch_norm_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 20, 20, 1024) 263168      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_58 (BatchNorm)       (None, 20, 20, 1024) 4096        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 20, 20, 1024) 0           batch_norm_58[0][0]              \n",
      "                                                                 activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 20, 20, 1024) 0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 20, 20, 256)  2359552     activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 20, 20, 256)  0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 20, 20, 8)    2056        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 2)      0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 20, 20, 16)   4112        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, None, 2)      0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 20, 20, 16)   0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, 2)      0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 4)      0           activation_59[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 11,726,616\n",
      "Trainable params: 11,690,648\n",
      "Non-trainable params: 35,968\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = KL.Input((config.image_size[0], config.image_size[0], 3))  # change\n",
    "fp = resNet_featureExtractor(x)\n",
    "rpn_class, rpn_prob, rpn_bbox = rpn_net(fp, anchor_num)\n",
    "model = Model([x], [rpn_class, rpn_prob, rpn_bbox])\n",
    "model.summary()\n",
    "# plot_model(model, to_file=\"model/model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-ae7db8d23bf6>:15: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x：rpn_box，counts：anchor为1的个数，num_rows = len(counts)：总共多少个\n",
    "# 小函数，取出rpn_box前面有值的“偏移量”\n",
    "def batch_back(x, counts, num_rows):\n",
    "    outputs = []\n",
    "    for i in range(num_rows):\n",
    "        outputs.append(x[i, :counts[i]])\n",
    "    return tf.concat(outputs, axis=0)\n",
    "\n",
    "# 分类loss\n",
    "# rpn_match：真实下标  rpn_class_logits：实际计算出的结果\n",
    "def rpn_class_loss(rpn_match, rpn_class_logits):\n",
    "    # 压缩最后一维，变成一维的坐标，rpn_match (None, 576, 1) => (None, 576) \n",
    "    rpn_match = tf.squeeze(rpn_match, -1)\n",
    "    # 取出 -1 和 1 的坐标\n",
    "    indices = tf.where(K.not_equal(rpn_match, 0))\n",
    "    # 把 -1 和 0 变成 0， 1 还是 1\n",
    "    anchor_class = K.cast(K.equal(rpn_match, 1), tf.int32)\n",
    "    # 取出实际计算出的所有-1 和 1坐标的anchor\n",
    "    rpn_class_logits = tf.gather_nd(rpn_class_logits, indices)     ### prediction\n",
    "    # 取出真实下标的所有-1 和 1坐标的anchor => 现在是（0, 1）0：表示背景 1：表示前景\n",
    "    anchor_class = tf.gather_nd(anchor_class, indices)   ### target\n",
    "    # 计算loss\n",
    "    loss = K.sparse_categorical_crossentropy(target=anchor_class, output=rpn_class_logits, from_logits=True)\n",
    "    # 计算平均，如果loss算出东西了，就取平均，如果没算出东西，就取0\n",
    "    loss = K.switch(tf.size(loss) > 0 , K.mean(loss), tf.constant(0.0))\n",
    "    return loss\n",
    "\n",
    "# 回归loss\n",
    "# target_bbox：真实的边框, rpn_match：真实的anchor下标, rpn_bbox：实际计算出来的结果边框\n",
    "def rpn_bbox_loss(target_bbox, rpn_match, rpn_bbox):\n",
    "    # 压缩最后一维，变成一维的坐标，rpn_match (None, 576, 1) => (None, 576) \n",
    "    rpn_match = tf.squeeze(rpn_match, -1)\n",
    "    # 取出 1 的坐标\n",
    "    indices = tf.where(K.equal(rpn_match, 1))\n",
    "    # 取出所有 1 的“偏移量”\n",
    "    rpn_bbox = tf.gather_nd(rpn_bbox, indices)\n",
    "    # 得出为 1 的“偏移量”的个数\n",
    "    batch_counts = K.sum(K.cast(K.equal(rpn_match, 1), tf.int32), axis=1)\n",
    "    # 取出 target_bbox 中为1的“偏移量”\n",
    "    target_bbox = batch_back(target_bbox, batch_counts, config.batch_size)\n",
    "    # 求误差\n",
    "    diff = K.abs(target_bbox - rpn_bbox)\n",
    "    # 取出小于1的部分\n",
    "    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n",
    "    # 小于1的部分给抛物线  diff**2\n",
    "    # 大于1的部分给直线（不会对大的误差敏感） diff-0.5\n",
    "    loss = (less_than_one * 0.5 * diff**2) + ((1 - less_than_one) * (diff - 0.5))\n",
    "    # 计算平均，如果loss算出东西了，就取平均，如果没算出东西，就取0\n",
    "    loss = K.switch(tf.size(loss) > 0 , K.mean(loss), tf.constant(0.0))\n",
    "    return loss\n",
    "\n",
    "# 图片的输入\n",
    "input_image = KL.Input(shape=[config.image_size[0], config.image_size[0],3], dtype=tf.float32)  # change\n",
    "# 真实的边框输入\n",
    "input_bboxes = KL.Input(shape=[None, 4], dtype=tf.float32)\n",
    "# 真实的分类输入\n",
    "input_class_ids = KL.Input(shape=[None], dtype=tf.int32)\n",
    "# 真实的anchor分类 -1,0,1输入\n",
    "input_rpn_match = KL.Input(shape=[None, 1], dtype=tf.int32)\n",
    "# 真实的anchor偏移量输入\n",
    "input_rpn_bbox = KL.Input(shape=[None, 4], dtype=tf.float32)\n",
    "\n",
    "\n",
    "# 创建模型\n",
    "feature_map = resNet_featureExtractor(input_image)\n",
    "# 模型计算出的结果 rpn_class，rpn_prob，rpn_bbox\n",
    "rpn_class, rpn_prob, rpn_bbox = rpn_net(feature_map, anchor_num)\n",
    "# 分类loss\n",
    "loss_rpn_match = KL.Lambda(lambda x: rpn_class_loss(*x), name=\"loss_rpn_match\")(\n",
    "    [input_rpn_match, rpn_class]\n",
    ")\n",
    "loss_rpn_bbox = KL.Lambda(lambda x:rpn_bbox_loss(*x), name=\"loss_rpn_bbox\")(\n",
    "    [input_rpn_bbox, input_rpn_match, rpn_bbox]\n",
    ")\n",
    "model = Model(\n",
    "    [input_image, input_bboxes, input_class_ids, input_rpn_match, input_rpn_bbox],\n",
    "    [rpn_class, rpn_prob, rpn_bbox, loss_rpn_match, loss_rpn_bbox]\n",
    ")\n",
    "# 添加loss层\n",
    "loss_lay1 = model.get_layer(\"loss_rpn_match\").output\n",
    "loss_lay2 = model.get_layer(\"loss_rpn_bbox\").output\n",
    "model.add_loss(tf.reduce_mean(loss_lay1))\n",
    "model.add_loss(tf.reduce_mean(loss_lay2))\n",
    "# 编译\n",
    "model.compile(loss = [None] * len(model.output), optimizer=keras.optimizers.SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False))\n",
    "# 打印出两个loss的收敛情况\n",
    "model.metrics_names.append(\"loss_rpn_match\")\n",
    "model.metrics_tensors.append(tf.reduce_mean(loss_lay1, keep_dims=True))\n",
    "model.metrics_names.append(\"loss_rpn_bbox\")\n",
    "model.metrics_tensors.append(tf.reduce_mean(loss_lay2, keep_dims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_Gen(num_batch, batch_size, config):\n",
    "    print(\"----------------\")\n",
    "    for iii in range(num_batch):\n",
    "        images = []\n",
    "        bboxes = []\n",
    "        class_ids = []\n",
    "        rpn_matchs = []\n",
    "        rpn_bboxes = []\n",
    "        for i in range(batch_size):\n",
    "            image, bbox, class_id, rpn_match, rpn_bbox, _ = data = dataset.load_data()\n",
    "            pad_num = config.max_gt_obj - bbox.shape[0]\n",
    "            pad_box = np.zeros((pad_num, 4))\n",
    "            pad_ids = np.zeros((pad_num, 1))\n",
    "            bbox = np.concatenate([bbox, pad_box], axis=0)\n",
    "            class_id = np.concatenate([class_id, pad_ids], axis=0)\n",
    "        \n",
    "            images.append(image)\n",
    "            bboxes.append(bbox)\n",
    "            class_ids.append(class_id)\n",
    "            rpn_matchs.append(rpn_match)\n",
    "            rpn_bboxes.append(rpn_bbox)\n",
    "\n",
    "#         print(\"数据：\" + str(index))\n",
    "        images = np.concatenate(images, 0).reshape(batch_size, config.image_size[0],config.image_size[1] , 3)\n",
    "        bboxes = np.concatenate(bboxes, 0).reshape(batch_size, -1 , 4)\n",
    "        class_ids = np.concatenate(class_ids, 0).reshape(batch_size, -1 )\n",
    "        rpn_matchs = np.concatenate(rpn_matchs, 0).reshape(batch_size, -1 , 1)\n",
    "        rpn_bboxes = np.concatenate(rpn_bboxes, 0).reshape(batch_size, -1 , 4)\n",
    "        yield [images, bboxes, class_ids, rpn_matchs, rpn_bboxes],[]\n",
    "\n",
    "total = 200000          # 总数据\n",
    "steps_per_epoch = 20   # 步长\n",
    "dataGen = data_Gen(int(total), config.batch_size, config) # 10000个数据，batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "----------------Epoch 1/200\n",
      "\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 279s 14s/step - loss: 6.2875 - loss_rpn_match: 2.2953 - loss_rpn_bbox: 3.9922\n",
      "Epoch 2/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 294s 15s/step - loss: 0.8813 - loss_rpn_match: 0.4107 - loss_rpn_bbox: 0.4705\n",
      "Epoch 3/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 273s 14s/step - loss: 0.6454 - loss_rpn_match: 0.2753 - loss_rpn_bbox: 0.3702\n",
      "Epoch 4/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 296s 15s/step - loss: 0.6039 - loss_rpn_match: 0.2561 - loss_rpn_bbox: 0.3478\n",
      "Epoch 5/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 290s 15s/step - loss: 0.5749 - loss_rpn_match: 0.2414 - loss_rpn_bbox: 0.3334\n",
      "Epoch 6/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 287s 14s/step - loss: 0.5821 - loss_rpn_match: 0.2436 - loss_rpn_bbox: 0.3385\n",
      "Epoch 7/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 319s 16s/step - loss: 0.5788 - loss_rpn_match: 0.2396 - loss_rpn_bbox: 0.3392\n",
      "Epoch 8/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 305s 15s/step - loss: 0.5753 - loss_rpn_match: 0.2437 - loss_rpn_bbox: 0.3315\n",
      "Epoch 9/200\n",
      "学习率：0.0001\n",
      "20/20 [==============================] - 282s 14s/step - loss: 0.5623 - loss_rpn_match: 0.2341 - loss_rpn_bbox: 0.3283\n",
      "Epoch 10/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5613 - loss_rpn_match: 0.2351 - loss_rpn_bbox: 0.3262\n",
      "Epoch 11/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 253s 13s/step - loss: 0.5445 - loss_rpn_match: 0.2255 - loss_rpn_bbox: 0.3190\n",
      "Epoch 12/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 253s 13s/step - loss: 0.5472 - loss_rpn_match: 0.2296 - loss_rpn_bbox: 0.3176\n",
      "Epoch 13/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5605 - loss_rpn_match: 0.2341 - loss_rpn_bbox: 0.3265\n",
      "Epoch 14/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 253s 13s/step - loss: 0.5531 - loss_rpn_match: 0.2344 - loss_rpn_bbox: 0.3188\n",
      "Epoch 15/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 253s 13s/step - loss: 0.5727 - loss_rpn_match: 0.2445 - loss_rpn_bbox: 0.3282\n",
      "Epoch 16/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5464 - loss_rpn_match: 0.2271 - loss_rpn_bbox: 0.3192\n",
      "Epoch 17/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 254s 13s/step - loss: 0.5346 - loss_rpn_match: 0.2156 - loss_rpn_bbox: 0.3190\n",
      "Epoch 18/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 254s 13s/step - loss: 0.5334 - loss_rpn_match: 0.2193 - loss_rpn_bbox: 0.3140\n",
      "Epoch 19/200\n",
      "学习率：5e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5556 - loss_rpn_match: 0.2403 - loss_rpn_bbox: 0.3153\n",
      "Epoch 20/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 254s 13s/step - loss: 0.5495 - loss_rpn_match: 0.2241 - loss_rpn_bbox: 0.3253\n",
      "Epoch 21/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5430 - loss_rpn_match: 0.2290 - loss_rpn_bbox: 0.3141\n",
      "Epoch 22/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5395 - loss_rpn_match: 0.2277 - loss_rpn_bbox: 0.3118\n",
      "Epoch 23/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5418 - loss_rpn_match: 0.2256 - loss_rpn_bbox: 0.3162\n",
      "Epoch 24/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5524 - loss_rpn_match: 0.2279 - loss_rpn_bbox: 0.3245\n",
      "Epoch 25/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5329 - loss_rpn_match: 0.2240 - loss_rpn_bbox: 0.3089\n",
      "Epoch 26/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5314 - loss_rpn_match: 0.2171 - loss_rpn_bbox: 0.3143\n",
      "Epoch 27/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5467 - loss_rpn_match: 0.2278 - loss_rpn_bbox: 0.3189\n",
      "Epoch 28/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5489 - loss_rpn_match: 0.2304 - loss_rpn_bbox: 0.3184\n",
      "Epoch 29/200\n",
      "学习率：2.5e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5485 - loss_rpn_match: 0.2255 - loss_rpn_bbox: 0.3230\n",
      "Epoch 30/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5347 - loss_rpn_match: 0.2212 - loss_rpn_bbox: 0.3135\n",
      "Epoch 31/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5387 - loss_rpn_match: 0.2219 - loss_rpn_bbox: 0.3168\n",
      "Epoch 32/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5407 - loss_rpn_match: 0.2245 - loss_rpn_bbox: 0.3162\n",
      "Epoch 33/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5414 - loss_rpn_match: 0.2267 - loss_rpn_bbox: 0.3148\n",
      "Epoch 34/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5431 - loss_rpn_match: 0.2285 - loss_rpn_bbox: 0.3146\n",
      "Epoch 35/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5308 - loss_rpn_match: 0.2238 - loss_rpn_bbox: 0.3071\n",
      "Epoch 36/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5241 - loss_rpn_match: 0.2149 - loss_rpn_bbox: 0.3092\n",
      "Epoch 37/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5459 - loss_rpn_match: 0.2319 - loss_rpn_bbox: 0.3140\n",
      "Epoch 38/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5186 - loss_rpn_match: 0.2128 - loss_rpn_bbox: 0.3058\n",
      "Epoch 39/200\n",
      "学习率：1.25e-05\n",
      "20/20 [==============================] - 256s 13s/step - loss: 0.5456 - loss_rpn_match: 0.2305 - loss_rpn_bbox: 0.3151\n",
      "Epoch 40/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5382 - loss_rpn_match: 0.2250 - loss_rpn_bbox: 0.3132\n",
      "Epoch 41/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5360 - loss_rpn_match: 0.2215 - loss_rpn_bbox: 0.3146\n",
      "Epoch 42/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5463 - loss_rpn_match: 0.2325 - loss_rpn_bbox: 0.3138\n",
      "Epoch 43/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5531 - loss_rpn_match: 0.2300 - loss_rpn_bbox: 0.3231\n",
      "Epoch 44/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5341 - loss_rpn_match: 0.2259 - loss_rpn_bbox: 0.3083\n",
      "Epoch 45/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 255s 13s/step - loss: 0.5351 - loss_rpn_match: 0.2222 - loss_rpn_bbox: 0.3129\n",
      "Epoch 46/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5265 - loss_rpn_match: 0.2210 - loss_rpn_bbox: 0.3055\n",
      "Epoch 47/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5321 - loss_rpn_match: 0.2191 - loss_rpn_bbox: 0.3131\n",
      "Epoch 48/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5379 - loss_rpn_match: 0.2265 - loss_rpn_bbox: 0.3114\n",
      "Epoch 49/200\n",
      "学习率：6.25e-06\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5364 - loss_rpn_match: 0.2233 - loss_rpn_bbox: 0.3131\n",
      "Epoch 50/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5402 - loss_rpn_match: 0.2221 - loss_rpn_bbox: 0.3180\n",
      "Epoch 51/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5453 - loss_rpn_match: 0.2280 - loss_rpn_bbox: 0.3172\n",
      "Epoch 52/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5369 - loss_rpn_match: 0.2244 - loss_rpn_bbox: 0.3124\n",
      "Epoch 53/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5350 - loss_rpn_match: 0.2220 - loss_rpn_bbox: 0.3130\n",
      "Epoch 54/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5268 - loss_rpn_match: 0.2199 - loss_rpn_bbox: 0.3070\n",
      "Epoch 55/200\n",
      "学习率：3.125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 261s 13s/step - loss: 0.5352 - loss_rpn_match: 0.2232 - loss_rpn_bbox: 0.3120\n",
      "Epoch 56/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5302 - loss_rpn_match: 0.2193 - loss_rpn_bbox: 0.3109\n",
      "Epoch 57/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5318 - loss_rpn_match: 0.2221 - loss_rpn_bbox: 0.3097\n",
      "Epoch 58/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 263s 13s/step - loss: 0.5244 - loss_rpn_match: 0.2175 - loss_rpn_bbox: 0.3069\n",
      "Epoch 59/200\n",
      "学习率：3.125e-06\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5289 - loss_rpn_match: 0.2184 - loss_rpn_bbox: 0.3106\n",
      "Epoch 60/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5255 - loss_rpn_match: 0.2204 - loss_rpn_bbox: 0.3052\n",
      "Epoch 61/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5445 - loss_rpn_match: 0.2311 - loss_rpn_bbox: 0.3134\n",
      "Epoch 62/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5182 - loss_rpn_match: 0.2151 - loss_rpn_bbox: 0.3031\n",
      "Epoch 63/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5376 - loss_rpn_match: 0.2241 - loss_rpn_bbox: 0.3135\n",
      "Epoch 64/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5350 - loss_rpn_match: 0.2293 - loss_rpn_bbox: 0.3057\n",
      "Epoch 65/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5312 - loss_rpn_match: 0.2230 - loss_rpn_bbox: 0.3082\n",
      "Epoch 66/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5392 - loss_rpn_match: 0.2269 - loss_rpn_bbox: 0.3123\n",
      "Epoch 67/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5385 - loss_rpn_match: 0.2245 - loss_rpn_bbox: 0.3140\n",
      "Epoch 68/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5399 - loss_rpn_match: 0.2224 - loss_rpn_bbox: 0.3175\n",
      "Epoch 69/200\n",
      "学习率：1.5625e-06\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5291 - loss_rpn_match: 0.2217 - loss_rpn_bbox: 0.3074\n",
      "Epoch 70/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 263s 13s/step - loss: 0.5344 - loss_rpn_match: 0.2219 - loss_rpn_bbox: 0.3125\n",
      "Epoch 71/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 263s 13s/step - loss: 0.5166 - loss_rpn_match: 0.2121 - loss_rpn_bbox: 0.3044\n",
      "Epoch 72/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 264s 13s/step - loss: 0.5186 - loss_rpn_match: 0.2139 - loss_rpn_bbox: 0.3047\n",
      "Epoch 73/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5378 - loss_rpn_match: 0.2290 - loss_rpn_bbox: 0.3089\n",
      "Epoch 74/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5508 - loss_rpn_match: 0.2384 - loss_rpn_bbox: 0.3124\n",
      "Epoch 75/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5408 - loss_rpn_match: 0.2259 - loss_rpn_bbox: 0.3149\n",
      "Epoch 76/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5296 - loss_rpn_match: 0.2194 - loss_rpn_bbox: 0.3102\n",
      "Epoch 77/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5352 - loss_rpn_match: 0.2230 - loss_rpn_bbox: 0.3123\n",
      "Epoch 78/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5585 - loss_rpn_match: 0.2360 - loss_rpn_bbox: 0.3225\n",
      "Epoch 79/200\n",
      "学习率：7.8125e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5331 - loss_rpn_match: 0.2249 - loss_rpn_bbox: 0.3082\n",
      "Epoch 80/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5498 - loss_rpn_match: 0.2306 - loss_rpn_bbox: 0.3192\n",
      "Epoch 81/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5305 - loss_rpn_match: 0.2194 - loss_rpn_bbox: 0.3111\n",
      "Epoch 82/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5246 - loss_rpn_match: 0.2154 - loss_rpn_bbox: 0.3091\n",
      "Epoch 83/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5404 - loss_rpn_match: 0.2229 - loss_rpn_bbox: 0.3175\n",
      "Epoch 84/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5306 - loss_rpn_match: 0.2171 - loss_rpn_bbox: 0.3135\n",
      "Epoch 85/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5354 - loss_rpn_match: 0.2231 - loss_rpn_bbox: 0.3123\n",
      "Epoch 86/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5414 - loss_rpn_match: 0.2313 - loss_rpn_bbox: 0.3102\n",
      "Epoch 87/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5382 - loss_rpn_match: 0.2194 - loss_rpn_bbox: 0.3188\n",
      "Epoch 88/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5245 - loss_rpn_match: 0.2161 - loss_rpn_bbox: 0.3084\n",
      "Epoch 89/200\n",
      "学习率：3.90625e-07\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5379 - loss_rpn_match: 0.2256 - loss_rpn_bbox: 0.3124\n",
      "Epoch 90/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5262 - loss_rpn_match: 0.2193 - loss_rpn_bbox: 0.3069\n",
      "Epoch 91/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5389 - loss_rpn_match: 0.2299 - loss_rpn_bbox: 0.3089\n",
      "Epoch 92/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5410 - loss_rpn_match: 0.2290 - loss_rpn_bbox: 0.3121\n",
      "Epoch 93/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5323 - loss_rpn_match: 0.2254 - loss_rpn_bbox: 0.3068\n",
      "Epoch 94/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5198 - loss_rpn_match: 0.2210 - loss_rpn_bbox: 0.2988\n",
      "Epoch 95/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5334 - loss_rpn_match: 0.2243 - loss_rpn_bbox: 0.3092\n",
      "Epoch 96/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5157 - loss_rpn_match: 0.2121 - loss_rpn_bbox: 0.3037\n",
      "Epoch 97/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5274 - loss_rpn_match: 0.2202 - loss_rpn_bbox: 0.3072\n",
      "Epoch 98/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5371 - loss_rpn_match: 0.2267 - loss_rpn_bbox: 0.3105\n",
      "Epoch 99/200\n",
      "学习率：1.953125e-07\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5290 - loss_rpn_match: 0.2184 - loss_rpn_bbox: 0.3106\n",
      "Epoch 100/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5415 - loss_rpn_match: 0.2317 - loss_rpn_bbox: 0.3098\n",
      "Epoch 101/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5473 - loss_rpn_match: 0.2293 - loss_rpn_bbox: 0.3180\n",
      "Epoch 102/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5364 - loss_rpn_match: 0.2228 - loss_rpn_bbox: 0.3137\n",
      "Epoch 103/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5337 - loss_rpn_match: 0.2296 - loss_rpn_bbox: 0.3041\n",
      "Epoch 104/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5355 - loss_rpn_match: 0.2187 - loss_rpn_bbox: 0.3167\n",
      "Epoch 105/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5476 - loss_rpn_match: 0.2333 - loss_rpn_bbox: 0.3143\n",
      "Epoch 106/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5318 - loss_rpn_match: 0.2168 - loss_rpn_bbox: 0.3150\n",
      "Epoch 107/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5291 - loss_rpn_match: 0.2178 - loss_rpn_bbox: 0.3112\n",
      "Epoch 108/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5338 - loss_rpn_match: 0.2195 - loss_rpn_bbox: 0.3143\n",
      "Epoch 109/200\n",
      "学习率：9.765625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5308 - loss_rpn_match: 0.2242 - loss_rpn_bbox: 0.3065\n",
      "Epoch 110/200\n",
      "学习率：4.8828125e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 259s 13s/step - loss: 0.5346 - loss_rpn_match: 0.2234 - loss_rpn_bbox: 0.3113\n",
      "Epoch 111/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5302 - loss_rpn_match: 0.2233 - loss_rpn_bbox: 0.3069\n",
      "Epoch 112/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5322 - loss_rpn_match: 0.2230 - loss_rpn_bbox: 0.3093\n",
      "Epoch 113/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5388 - loss_rpn_match: 0.2302 - loss_rpn_bbox: 0.3086\n",
      "Epoch 114/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5363 - loss_rpn_match: 0.2234 - loss_rpn_bbox: 0.3129\n",
      "Epoch 115/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5409 - loss_rpn_match: 0.2234 - loss_rpn_bbox: 0.3175\n",
      "Epoch 116/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 257s 13s/step - loss: 0.5337 - loss_rpn_match: 0.2227 - loss_rpn_bbox: 0.3110\n",
      "Epoch 117/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5279 - loss_rpn_match: 0.2202 - loss_rpn_bbox: 0.3077\n",
      "Epoch 118/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5289 - loss_rpn_match: 0.2193 - loss_rpn_bbox: 0.3096\n",
      "Epoch 119/200\n",
      "学习率：4.8828125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5264 - loss_rpn_match: 0.2202 - loss_rpn_bbox: 0.3062\n",
      "Epoch 120/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5491 - loss_rpn_match: 0.2316 - loss_rpn_bbox: 0.3174\n",
      "Epoch 121/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5408 - loss_rpn_match: 0.2256 - loss_rpn_bbox: 0.3152\n",
      "Epoch 122/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5152 - loss_rpn_match: 0.2105 - loss_rpn_bbox: 0.3047\n",
      "Epoch 123/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5302 - loss_rpn_match: 0.2196 - loss_rpn_bbox: 0.3106\n",
      "Epoch 124/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5338 - loss_rpn_match: 0.2275 - loss_rpn_bbox: 0.3063\n",
      "Epoch 125/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5389 - loss_rpn_match: 0.2245 - loss_rpn_bbox: 0.3144\n",
      "Epoch 126/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5346 - loss_rpn_match: 0.2264 - loss_rpn_bbox: 0.3081\n",
      "Epoch 127/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5317 - loss_rpn_match: 0.2215 - loss_rpn_bbox: 0.3102\n",
      "Epoch 128/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5454 - loss_rpn_match: 0.2285 - loss_rpn_bbox: 0.3169\n",
      "Epoch 129/200\n",
      "学习率：2.44140625e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5374 - loss_rpn_match: 0.2243 - loss_rpn_bbox: 0.3131\n",
      "Epoch 130/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5177 - loss_rpn_match: 0.2151 - loss_rpn_bbox: 0.3026\n",
      "Epoch 131/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5222 - loss_rpn_match: 0.2107 - loss_rpn_bbox: 0.3115\n",
      "Epoch 132/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5349 - loss_rpn_match: 0.2247 - loss_rpn_bbox: 0.3102\n",
      "Epoch 133/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5172 - loss_rpn_match: 0.2143 - loss_rpn_bbox: 0.3029\n",
      "Epoch 134/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5286 - loss_rpn_match: 0.2193 - loss_rpn_bbox: 0.3093\n",
      "Epoch 135/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5418 - loss_rpn_match: 0.2251 - loss_rpn_bbox: 0.3167\n",
      "Epoch 136/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 262s 13s/step - loss: 0.5397 - loss_rpn_match: 0.2230 - loss_rpn_bbox: 0.3167\n",
      "Epoch 137/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5350 - loss_rpn_match: 0.2251 - loss_rpn_bbox: 0.3098\n",
      "Epoch 138/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5407 - loss_rpn_match: 0.2307 - loss_rpn_bbox: 0.3100\n",
      "Epoch 139/200\n",
      "学习率：1.220703125e-08\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5282 - loss_rpn_match: 0.2211 - loss_rpn_bbox: 0.3071\n",
      "Epoch 140/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5274 - loss_rpn_match: 0.2177 - loss_rpn_bbox: 0.3096\n",
      "Epoch 141/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5387 - loss_rpn_match: 0.2280 - loss_rpn_bbox: 0.3107\n",
      "Epoch 142/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5432 - loss_rpn_match: 0.2302 - loss_rpn_bbox: 0.3130\n",
      "Epoch 143/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5364 - loss_rpn_match: 0.2300 - loss_rpn_bbox: 0.3065\n",
      "Epoch 144/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5171 - loss_rpn_match: 0.2171 - loss_rpn_bbox: 0.3000\n",
      "Epoch 145/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5414 - loss_rpn_match: 0.2267 - loss_rpn_bbox: 0.3148\n",
      "Epoch 146/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5387 - loss_rpn_match: 0.2242 - loss_rpn_bbox: 0.3145\n",
      "Epoch 147/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5328 - loss_rpn_match: 0.2225 - loss_rpn_bbox: 0.3103\n",
      "Epoch 148/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5246 - loss_rpn_match: 0.2194 - loss_rpn_bbox: 0.3052\n",
      "Epoch 149/200\n",
      "学习率：6.103515625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5298 - loss_rpn_match: 0.2241 - loss_rpn_bbox: 0.3057\n",
      "Epoch 150/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5313 - loss_rpn_match: 0.2236 - loss_rpn_bbox: 0.3077\n",
      "Epoch 151/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5441 - loss_rpn_match: 0.2290 - loss_rpn_bbox: 0.3151\n",
      "Epoch 152/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5309 - loss_rpn_match: 0.2204 - loss_rpn_bbox: 0.3106\n",
      "Epoch 153/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5437 - loss_rpn_match: 0.2333 - loss_rpn_bbox: 0.3104\n",
      "Epoch 154/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5183 - loss_rpn_match: 0.2134 - loss_rpn_bbox: 0.3049\n",
      "Epoch 155/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5382 - loss_rpn_match: 0.2235 - loss_rpn_bbox: 0.3148\n",
      "Epoch 156/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5319 - loss_rpn_match: 0.2206 - loss_rpn_bbox: 0.3113\n",
      "Epoch 157/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5295 - loss_rpn_match: 0.2224 - loss_rpn_bbox: 0.3071\n",
      "Epoch 158/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5359 - loss_rpn_match: 0.2236 - loss_rpn_bbox: 0.3124\n",
      "Epoch 159/200\n",
      "学习率：3.0517578125e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5307 - loss_rpn_match: 0.2234 - loss_rpn_bbox: 0.3072\n",
      "Epoch 160/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5368 - loss_rpn_match: 0.2301 - loss_rpn_bbox: 0.3067\n",
      "Epoch 161/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5560 - loss_rpn_match: 0.2376 - loss_rpn_bbox: 0.3185\n",
      "Epoch 162/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5264 - loss_rpn_match: 0.2199 - loss_rpn_bbox: 0.3065\n",
      "Epoch 163/200\n",
      "学习率：1.52587890625e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 259s 13s/step - loss: 0.5334 - loss_rpn_match: 0.2216 - loss_rpn_bbox: 0.3118\n",
      "Epoch 164/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5363 - loss_rpn_match: 0.2254 - loss_rpn_bbox: 0.3109\n",
      "Epoch 165/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5261 - loss_rpn_match: 0.2169 - loss_rpn_bbox: 0.3092\n",
      "Epoch 166/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5358 - loss_rpn_match: 0.2217 - loss_rpn_bbox: 0.3141\n",
      "Epoch 167/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5281 - loss_rpn_match: 0.2202 - loss_rpn_bbox: 0.3079\n",
      "Epoch 168/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5229 - loss_rpn_match: 0.2165 - loss_rpn_bbox: 0.3064\n",
      "Epoch 169/200\n",
      "学习率：1.52587890625e-09\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5323 - loss_rpn_match: 0.2236 - loss_rpn_bbox: 0.3087\n",
      "Epoch 170/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5507 - loss_rpn_match: 0.2333 - loss_rpn_bbox: 0.3174\n",
      "Epoch 171/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5446 - loss_rpn_match: 0.2285 - loss_rpn_bbox: 0.3161\n",
      "Epoch 172/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5401 - loss_rpn_match: 0.2288 - loss_rpn_bbox: 0.3112\n",
      "Epoch 173/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5476 - loss_rpn_match: 0.2283 - loss_rpn_bbox: 0.3193\n",
      "Epoch 174/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5319 - loss_rpn_match: 0.2217 - loss_rpn_bbox: 0.3103\n",
      "Epoch 175/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5416 - loss_rpn_match: 0.2203 - loss_rpn_bbox: 0.3212\n",
      "Epoch 176/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5306 - loss_rpn_match: 0.2279 - loss_rpn_bbox: 0.3027\n",
      "Epoch 177/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5369 - loss_rpn_match: 0.2201 - loss_rpn_bbox: 0.3167\n",
      "Epoch 178/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5305 - loss_rpn_match: 0.2192 - loss_rpn_bbox: 0.3113\n",
      "Epoch 179/200\n",
      "学习率：7.62939453125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5334 - loss_rpn_match: 0.2215 - loss_rpn_bbox: 0.3119\n",
      "Epoch 180/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5366 - loss_rpn_match: 0.2265 - loss_rpn_bbox: 0.3101\n",
      "Epoch 181/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5265 - loss_rpn_match: 0.2212 - loss_rpn_bbox: 0.3053\n",
      "Epoch 182/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5327 - loss_rpn_match: 0.2259 - loss_rpn_bbox: 0.3067\n",
      "Epoch 183/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5338 - loss_rpn_match: 0.2197 - loss_rpn_bbox: 0.3141\n",
      "Epoch 184/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5327 - loss_rpn_match: 0.2229 - loss_rpn_bbox: 0.3098\n",
      "Epoch 185/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5418 - loss_rpn_match: 0.2273 - loss_rpn_bbox: 0.3144\n",
      "Epoch 186/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5428 - loss_rpn_match: 0.2241 - loss_rpn_bbox: 0.3187\n",
      "Epoch 187/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5460 - loss_rpn_match: 0.2277 - loss_rpn_bbox: 0.3182\n",
      "Epoch 188/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5339 - loss_rpn_match: 0.2213 - loss_rpn_bbox: 0.3127\n",
      "Epoch 189/200\n",
      "学习率：3.814697265625e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5318 - loss_rpn_match: 0.2216 - loss_rpn_bbox: 0.3102\n",
      "Epoch 190/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5419 - loss_rpn_match: 0.2314 - loss_rpn_bbox: 0.3105\n",
      "Epoch 191/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5348 - loss_rpn_match: 0.2245 - loss_rpn_bbox: 0.3103\n",
      "Epoch 192/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5260 - loss_rpn_match: 0.2210 - loss_rpn_bbox: 0.3050\n",
      "Epoch 193/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5336 - loss_rpn_match: 0.2217 - loss_rpn_bbox: 0.3119\n",
      "Epoch 194/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5310 - loss_rpn_match: 0.2223 - loss_rpn_bbox: 0.3087\n",
      "Epoch 195/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5323 - loss_rpn_match: 0.2200 - loss_rpn_bbox: 0.3124\n",
      "Epoch 196/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5274 - loss_rpn_match: 0.2238 - loss_rpn_bbox: 0.3036\n",
      "Epoch 197/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 260s 13s/step - loss: 0.5329 - loss_rpn_match: 0.2212 - loss_rpn_bbox: 0.3116\n",
      "Epoch 198/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 258s 13s/step - loss: 0.5320 - loss_rpn_match: 0.2185 - loss_rpn_bbox: 0.3135\n",
      "Epoch 199/200\n",
      "学习率：1.9073486328125e-10\n",
      "20/20 [==============================] - 259s 13s/step - loss: 0.5336 - loss_rpn_match: 0.2199 - loss_rpn_bbox: 0.3137\n",
      "Epoch 200/200\n",
      "学习率：9.5367431640625e-11\n",
      "20/20 [==============================] - 261s 13s/step - loss: 0.5299 - loss_rpn_match: 0.2219 - loss_rpn_bbox: 0.3080\n"
     ]
    }
   ],
   "source": [
    "# face V2版本\n",
    "# 参数1320万 320 × 320  batch_size：10 steps_per_epoch：20  rpn_stride：16  scales：[55, 60, 70, 85, 105, 140, 180, 230]  buildblock：[7,7,7]   文件名：model_224_16_[epochs数]_v2.h5\n",
    "# 第一次20epochs lr=0.0001 batch_size：10  1w数据\n",
    "# drop = 0.5 衰减率调小\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    print('学习率：' + str(lrate))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "his = model.fit_generator(dataGen, steps_per_epoch=steps_per_epoch, epochs=200, callbacks=[lrate])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'model_material_64_200.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-15e09cca4600>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model.save_weights(\"model_material_64_200.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_material_64_200.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# model.save_weights(\"model_material_160_200.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# model.load_weights(\"model_material1.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   2656\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`load_weights` requires h5py.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2658\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2659\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'layer_names'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'model_weights'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'model_material_64_200.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# model.save_weights(\"model_material_64_200.h5\")\n",
    "model.load_weights(\"model_material_64_200.h5\")\n",
    "# model.save_weights(\"model_material_160_200.h5\")\n",
    "# model.load_weights(\"model_material1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将anchors根据计算出的修正量逆运算出实际计算的anchors\n",
    "# anchors：锚框\n",
    "# deltas：网络计算出的修正量\n",
    "def anchor_refinement(anchors, deltas):\n",
    "    boxes = tf.cast(anchors, tf.float32)\n",
    "    h = boxes[:, 2] - boxes[:, 0]\n",
    "    w = boxes[:, 3] - boxes[:, 1]\n",
    "    center_y = boxes[:, 0] + h / 2\n",
    "    center_x = boxes[:, 1] + w / 2\n",
    "\n",
    "    center_y += deltas[:, 0] * h\n",
    "    center_x += deltas[:, 1] * w\n",
    "    h *= tf.exp(deltas[:, 2])\n",
    "    w *= tf.exp(deltas[:, 3])\n",
    "    \n",
    "    y1 = center_y - h / 2\n",
    "    x1 = center_x - w / 2\n",
    "    y2 = center_y + h / 2\n",
    "    x2 = center_x + w / 2\n",
    "    boxes = tf.stack([y1, x1, y2, x2], axis=1)\n",
    "    return boxes\n",
    "\n",
    "# 限定坐标的边界，不超过边界\n",
    "def boxes_clip(boxes, window):\n",
    "    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n",
    "    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n",
    "    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n",
    "    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n",
    "    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n",
    "    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n",
    "    cliped = tf.concat([y1, x1, y2, x2], axis=1)\n",
    "    cliped.set_shape((cliped.shape[0], 4))\n",
    "    return cliped\n",
    "\n",
    "# 将输入的inputs 按提供的 graph_fn方法进行切片\n",
    "# 因为数据是batch_size个的数组，所以这个函数是进行batch_size个数组的操作\n",
    "def batch_slice(inputs, graph_fn, batch_size):\n",
    "    if not isinstance(inputs, list):\n",
    "        inputs = [inputs]\n",
    "    output = []\n",
    "    for i in range(batch_size):\n",
    "        inputs_slice = [x[i] for x in inputs]\n",
    "        output_slice = graph_fn(*inputs_slice)\n",
    "        if not isinstance(output_slice, (list, tuple)):\n",
    "            output_slice = [output_slice]\n",
    "        output.append(output_slice)\n",
    "    # 打包输出出去\n",
    "    output = list(zip(*output))\n",
    "    result = [tf.stack(o, axis=0) for o in output]\n",
    "    if len(result)==1:\n",
    "        result = result[0]\n",
    "    return result\n",
    "\n",
    "\n",
    "import keras.engine as KE\n",
    "class proposal(KE.Layer):\n",
    "    # proposal_count：保留的框个数\n",
    "    # nms_thresh：超过这个阈值，两个anchor就去pk\n",
    "    # anchors：锚框\n",
    "    # batch_size\n",
    "    # config：配置项\n",
    "    # kwargs：其他参数\n",
    "    def __init__(self, proposal_count, nms_thresh, anchors, batch_size, config=None, **kwargs):\n",
    "        super(proposal, self).__init__(**kwargs)\n",
    "        self.proposal_count = proposal_count\n",
    "        self.anchors = anchors\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.batch_size = batch_size\n",
    "        self.config = config\n",
    "    \n",
    "    # input：输入的 probs置信度inputs[0] 和 deltas修正量inputs[1]\n",
    "    def call(self, inputs):\n",
    "        # 取最后一维（前景概率）\n",
    "        probs = inputs[0][:, :, 1]\n",
    "        # 修正量\n",
    "        deltas = inputs[1]\n",
    "        # -------------------------------- 取得分前100的锚框数据 --------------------------------\n",
    "        # 乘RPN_BBOX_STD_DEV，是因为build_rpnTarget函数里面除以了这个数，避免值太小\n",
    "        deltas = deltas * np.reshape(self.config.RPN_BBOX_STD_DEV, (1, 1, 4))\n",
    "        # 判断：取100和anchors数之间最小\n",
    "        prenms_num = min(100, self.anchors.shape[0])\n",
    "        # 取前100个锚框得分最高的锚框下标\n",
    "        idxs = tf.nn.top_k(probs, prenms_num).indices\n",
    "        \n",
    "        # 按坐标取元素\n",
    "        # 置信度\n",
    "        probs = batch_slice([probs, idxs], lambda x,y:tf.gather(x, y), self.batch_size)\n",
    "        # 修正量\n",
    "        deltas = batch_slice([deltas, idxs], lambda x,y:tf.gather(x, y), self.batch_size)\n",
    "        # 锚框\n",
    "        anchors = batch_slice([idxs], lambda x:tf.gather(self.anchors,x), self.batch_size)\n",
    "\n",
    "        # ----------------------------------------- 修正 -----------------------------------------\n",
    "        # 取修正框\n",
    "        refined_boxes = batch_slice([anchors, deltas], lambda x,y:anchor_refinement(x,y), self.batch_size)\n",
    "        # 限定坐标的边界，不超过边界，避免修到外面去\n",
    "        H,W = self.config.image_size[:2]\n",
    "        windows = np.array([0,0,H,W]).astype(np.float32)\n",
    "        cliped_boxes = batch_slice([refined_boxes], lambda x:boxes_clip(x, windows), self.batch_size)\n",
    "        \n",
    "        # 归一化\n",
    "        normalized_boxes = cliped_boxes / np.array([H,W,H,W])\n",
    "        # 非极大值抑制方法\n",
    "        def nms(normalized_boxes, scores):\n",
    "            idxs_ = tf.image.non_max_suppression(normalized_boxes, scores, self.proposal_count, self.nms_thresh)\n",
    "            box = tf.gather(normalized_boxes, idxs_)\n",
    "            pad_num = tf.maximum(self.proposal_count - tf.shape(normalized_boxes)[0],0)\n",
    "            box = tf.pad(box, [(0,pad_num),(0,0)])\n",
    "            return box\n",
    "        # 进行非极大值抑制切换\n",
    "        proposal_ = batch_slice([normalized_boxes, probs], nms, self.batch_size)\n",
    "        return proposal_\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.proposal_count, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 160, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "test_data = next(dataGen)[0]\n",
    "images = test_data[0]\n",
    "bboxes = test_data[1]\n",
    "class_ids = test_data[2]\n",
    "rpn_matchs = test_data[3]\n",
    "rpn_bboxes = test_data[4]\n",
    "print(images.shape)\n",
    "rpn_class, rpn_prob, rpn_bbox, _, _ = \\\n",
    "                model.predict([images, bboxes, class_ids, rpn_matchs, rpn_bboxes])\n",
    "# 转tensor\n",
    "rpn_class = tf.convert_to_tensor(rpn_class)\n",
    "rpn_prob = tf.convert_to_tensor(rpn_prob)\n",
    "rpn_bbox = tf.convert_to_tensor(rpn_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "# change\n",
    "anchors = utils.anchor_gen(config.featureMap_size, ratios=config.ratios, scales=config.scales, rpn_stride=config.rpn_stride,\n",
    "                           anchor_stride = config.anchor_stride)\n",
    "proposals = proposal(proposal_count=15, nms_thresh=0.7, anchors=anchors, batch_size=config.batch_size, config=config)([rpn_prob, rpn_bbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "proposals_ = sess.run(proposals) * image_size # change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# ix = 2\n",
    "ix = random.sample(range(config.batch_size), 1)[0]\n",
    "proposal_ = proposals_[ix]\n",
    "img = images[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 88.22589     0.        120.78874    24.327328 ]\n",
      " [ 88.854324   39.48102   121.2169     71.62369  ]\n",
      " [117.997284    1.2314062 152.48703    32.90473  ]\n",
      " [ 56.187523    0.         88.72766    18.091991 ]\n",
      " [ 88.11301    30.67647   121.97856    64.76978  ]\n",
      " [ 54.091255   24.345097   91.00427    54.309982 ]\n",
      " [  1.6411457 122.68082    29.641134  146.93152  ]\n",
      " [ 87.48834    23.67511   122.63051    55.749107 ]\n",
      " [ 86.818726    0.        123.258835   33.175278 ]\n",
      " [ 88.484116   62.883553  120.81601    96.43243  ]\n",
      " [ 85.98894     6.8491764 122.128815   40.512985 ]\n",
      " [ 56.08635    31.217527   88.63289    62.953186 ]\n",
      " [ 78.101654  134.02054   110.45192   160.       ]\n",
      " [121.29146     0.        152.19926    25.433083 ]\n",
      " [ 43.812733   43.293144   68.16989    66.592094 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(img)\n",
    "axs = plt.gca()\n",
    "print(proposal_)\n",
    "for i in range(10):\n",
    "    box = proposal_[i]\n",
    "    rec = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], facecolor='none', edgecolor='r')\n",
    "    axs.add_patch(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
